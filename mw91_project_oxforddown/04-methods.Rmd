---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
documentclass: book
bibliography: references.bib
---

# Methoden {#methods} 

Diese Arbeit vergleicht die Vorhersagequalität der in der Accounting Fraud Detection gängigen logistischen Regression nach Dechow et al. (vgl. Bao et al. 2020, S. 2) mit der von neuronalen Netzen.
Die logistische Regression ist eng mit der linearen Regression verwandt und wird zur binären Schätzung einer Klassenzugehörigkeit verwandt (vgl. Géron 2019, S. 144). Dabei berechnet sie eine gewichtete Summe von Inputfaktoren und aggregiert sie zu einer Wahrscheinlichkeit zwischen 0 und 1. Liegt diese Wahrscheinlichkeit bei mindestens 0.5, so wird die Klasse „1“ vorhergesagt, welcher in dieser Arbeit der Klasse „fraud“ entspricht (vgl. Géron 2019, S. 144). Hierzu wird für jedes Attribut einer Beobachtung eine Sigmoid-Funktion verwendet, welche S-förmig vom Minimum bis zum Maximum des jeweiligen Attributs verläuft und die Verteilungen der Merkmalsausprägungen möglichst gut nach Klassenzugehörigkeit abgrenzt. Je weiter eine Merkmalsausprägung von dieser Grenze entfernt ist, desto näher ist der Funktionswert an der 1 oder der 0 (vgl. Géron 2019, S. 148).
Neuronale Netze bestehen aus drei Sorten von Schichten: Input-Schichten, welche Daten einlesen, versteckte Schichten, welche die Daten verarbeiten und Output-Schichten, welche aus den verarbeiteten Daten eine Prognose ableiten (vgl. Géron 2019, S. 286). In dieser Arbeit besteht die Output-Schicht aus lediglich einem Knoten, welche die Klassen „fraud“ abbildet. Die Anzahl der Knoten der Input-Schicht entspricht der Anzahl der Variablen im Datensatz. Die Knoten einer Schicht sind jeweils mit jedem Knoten seiner nachfolgenden Schicht durch „Gewichte“ verbunden. Jeder einzelne Knoten aggregiert die Signale, die er empfängt über deren Gewichte zu einer Zahl und wendet eine Aktivierungsfunktion an (vgl. Géron 2019, S. 282). Übersteigt der Funktionswert einen gegeben Schwellenwert, so „feuert“ das Neuron, was bedeutet, dass es ein Signal größer 0 an die Neuronen der nächsten Schicht weitergibt (vgl. Géron 2019, S. 282-283). Die Gewichte und alle Schwellenwerte werden durch Backpropagation unter Zuhilfenahme des Gradient Descent Algorithmus verbessert (vgl. Géron 2019, S.119 und S, 286). Sind alle Trainingsdaten einmal zum Training herangezogen worden, bedeutet das, dass das Netz für „eine Epoche“ trainiert wurde (vgl. Géron 2019, S. 127). In dieser Arbeit wird ein Netz über 100 Epochen hinweg trainiert.


 AB HIER HANDELT SICH DER AUFSCHRIEB MEHR UM NOTIZEN ALS UM EINE ABGABEFÄHIGE VERSION.
 
 
 
```{r, include=FALSE}
# install.packages("stringi", type="binary")
# install.packages('caret')
# install.packages('neuralnet')
# install.packages('dplyr')
# install.packages('Hmisc')
# install.packages('smotefamily')
# install.packages('readr')
# install.packages('rio', type='binary')
# install.packages('bookdown', type='binary')

```
```{r}

library(neuralnet)
library(caret)
library(dplyr)
library(Hmisc)
library(smotefamily)
library(readr)
library(rio)
```
 
 
```{r}
set.seed = 42

data <- import("data/uscecchini28.csv")
dim(data)
```
Wir haben 146045 Beobachtungen auf 51 Variablen. Wir müssen die fehlenden Werte Ientifizieren und die Spalten, welche zu viele davon enthalten löschen. Danach löschen wir die übrigen Zeilen, welche NaNs enthalten. Dies dient dem Umstand, dass NN mit NaN-Werten nicht arbeiten können. Wir verwenden für LogReg und NN den glichen Datensatz, um die Performance der Modelle vergleichen zu können. 
```{r}
nan_number_list <- vector()
nan_col_list <- vector()

for (col in colnames(data)){
  nan_number <- sum(is.na(data[col]))
  nan_number_list <- append(nan_number_list, nan_number, after = length(nan_number_list))
  nan_col_list <- append(nan_col_list, col, after = length(nan_col_list))
  nan_tabel <- as.data.frame(cbind(nan_number_list, nan_col_list))
  new_nan_tabel <- nan_tabel[order(- nan_number_list),]
  rownames(new_nan_tabel) <- NULL
}

print(head(new_nan_tabel, 10))

```

Die Variable, "p_aaer" und "new_p_aaer" fallen durch extrem viele NaN-Werte auf und werden komplett entfernt. Die übrigen Variablen bestehen zu maximal ca 15% aus NaN-Werten und werden daher als unbedenklich eingestuft. "new_p_aaer" und alle Beobachtungen, welche NaN-Werte enthalten, werden beseitigt. DAZU NOCH EINE EXTRASPALTE "nan_frac" EINFÜGEN UND nan_col_list ZUM INDEX MACHEN. nan_number_list zu "NaN-Werte".

```{r}
data <- data[, -match(c("new_p_aaer", "p_aaer"), names(data))]
data <- data[complete.cases(data),]
dim(data) # Übrige Beobachtungen: 116478 auf 49 Variablen
```
Damit ein NN oder die LogReg Inputdaten verwerten kann, müssen diese zwischen 0 und 1 normiert sein. 
```{r}
normalize <- function(col, na.rm = TRUE) {
    return((col- min(col)) /(max(col)-min(col)))
}
# as.data.frame(apply(df$name, normalize))
for (col in colnames(data))
{
  data[col] <- normalize(data[col])
}

```

trainings- und testdaten generieren (seed = immer noch 42)

```{r}
smp_size <- floor(0.70 * nrow(data))

train_ind <- sample(seq_len(nrow(data)), size = smp_size)

train <- data[train_ind, ]

true_frac <- sum(train[,7] == 1) / (smp_size - sum(train[,7] == 1)) #sagt aus, wie hoch Anteil der Betrugsfälle im Testdatenssatz ist


train_smote_object <- SMOTE(train[, -7], train[,7], K = 5, dup_size = 1 / true_frac)$data
train_smote_object$class <- as.numeric(train_smote_object$class)

test <- data[-train_ind, ]
```



```{r, include=TRUE}
#das sind nur nebenrechnungen

# Anteil der Betrugsfälle im Trainingsdatensatz vor SMOTE: 
sum(train[,7] == 1) / (smp_size - sum(train[,7] == 1))

# Anteil der Betrugsfälle im Trainingsdatensatz nach SMOTE: 
sum(train_smote_object$class == "1") / nrow(train_smote_object)

```

Jetzt wird ein NN trainiert und eine erste Prognose abgegeben. seed = 42
```{r}

nn <- neuralnet(
  class ~.,
  data = train_smote_object,
  hidden = 1,
  err.fct = 'sse',
  linear.output = FALSE,
  stepmax = 10000,
  lifesign = 'full',
  threshold = 7 # da sollte eigentlich 0.01 stehen. 7 ist unfug, aber da kommt man wenigstens hin. ich will schauen, ob das wenigstens einigermaßen okay ist. 
)
```
```{r}
plot(nn)
```
Nun werden die Ergebnisse für einen Score verwendet.

```{r}
# train[, !names(train) %in% c('misstate')] 
# das heißt "alle cols außer misstate".

output <- neuralnet::compute(nn, train_smote_object[, !names(train_smote_object) %in% c('class')])
head(output$net.result, 10)
head(train_smote_object[, names(train_smote_object) %in% c('class')], 10)
```
Klappt. Jetzt Confusion-Matrix:

```{r}
output <- neuralnet::compute(nn, train[, !names(train) %in% c('misstate')])

p1 <- output$net.result

pred1 <- ifelse(p1>0.5, 1, 0)

tab1 <- table(pred1, train$misstate)
tab1
```
okay, das ist immerhin mal etwas. 

```{r}
output_test <- neuralnet::compute(nn, test[, !names(test) %in% c('misstate')])

p2 <- output_test$net.result

pred2 <- ifelse(p2>0.5, 1, 0)

tab2 <- table(pred2, test$misstate)
tab2
```




FAZIT: Bisherige Vorhersage ist eine Katastrophe. Ich brauche ein komplexeres Netz.Rechenzeit geht aber durch die Decke ( mehrere Tage, wenn ich ein bisschen herumprobieren will).

Lösungsansätze: 

  1. [GESCHEITERT] Grafikkarte verwenden. Dazu Keras / Tensorflow instalieren, und herumprobieren. 
  2. NN verwerfen. -> NEIN! 
  3. PCA oder ElasticNet im Vorfeld verwenden, um Dimensionen zu reudzieren. -> Lieber nicht. So viele sinds nicht!
  4. (Hyper-) parameteropitimisierung:
      -> Mehr Layer / Nodes
      -> k=5 von SOMTE verändern. -> Nee, das geht schon so. 

Weiteres TODO: 
  -> LogReg basteln und versuchen, auf vernünftige Ergebnisse zu kommen. Baseline muss stehen.
  -> Funktion "evaluate" basteln, die mir die immer gleiche Tabelle ausgibt: Acc, Error, Prec, Sens, F1, F2
  -> PDF-Output muss für uns angepasst werden:
      -> HHU-Beschriftungen- und Bilder
      -> Unsere Namen
      -> Unsere Quellen & deren korrekte Zitation


```{r}
# Funktion "evaluate basteln. Rechnet mit y_test und y_pred. 
# Ausgabe ist Tabelle mit Acc, Error, Prec, Sens, F1, F2. 
# Die Metriken werden nur für die True-Zeile (also fraud) berechnet.

evaluate <- function(test, pred){
  
  pred <- ifelse(pred > 0.5, 1, 0)
  confusion <- table(test, pred)
  total_acc <- numeric(2)
  total_acc[1] <- NaN
  total_acc[2] <- round((confusion[1,1] + confusion[2,2]) / sum(confusion),4)

  
  prec <- numeric(2)
  prec[1] <- NaN
  prec[2] <- round(confusion[2,2] / (confusion[2,2] + confusion[2,1]),4)
  
  sens <- numeric(2)
  sens[1] <- NaN
  sens[2] <- round(confusion[2,2] / (confusion[2,2] + confusion[1,2]),4)
  
  F1 <- numeric(2)
  F1[1] <- NaN
  F1[2] <- round(2*(prec[2]*sens[2])/(prec[2] + sens[2]), 4)
  
  F2 <- numeric(2)
  F2[1] <- NaN
  F2[2] <- round((1 + 2^2) * (prec[2]*sens[2]) / (2^2 * prec[2] + sens[2]), 4)
  
  print(cbind(confusion, total_acc, prec, sens, F1, F2))
  
}


y_test <- c(1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1)
y_pred <- c(1,0,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1)

evaluate(test=y_test, pred=y_pred ) # sou, Evaluierungssfunktion steht. 
```

Ich wollte an der Stelle ein NN auf meiner Grafikkarte Trainieren. Aber es geht nicht. Nach 6h Arbeit und dem Versuch, RStudio dazu zu bringen, tensorflow zu installieren, gebe ich auf. Entweder wir nehmes das Netz von Sebbi oder ich trainiere nochmal eins alá c(16,16,8,8,4) oder so. Dafür den Rechner dann über Nacht stehen lassen. Problem dabei: Jedes fucking Mal, wenn wir hier nochmal etwas ändern wollen, muss das von vorne gemacht werden. Ich suche nach einer Möglichkeit, die Gewichte und Biases zu extrahieren und zu speichern. 

NEUES TODO:
  -> extract and save weights and biases after training. 

Jetzt erstmal die logistische Regression basteln:
```{r}
# trainingsdaten in train_smote_object
# testdaten in test

mylogit <- glm(class ~., data = train_smote_object, family = "binomial")
y_pred_logit <- predict.glm(mylogit, test[, !names(train) %in% c('misstate')])

evaluate(test$misstate, y_pred_logit)
```
Scheinbar ist irgendetwas perfekt in den Traingsdaten korreliert. Herausfinden was

```{r}
cor(select(train_smote_object, 'issue', 'class'))
```
```{r}
round(cor(train_smote_object$class, train_smote_object[, !names(train_smote_object) %in% c('class')]), 2)
```
GEFUNDEN: ch_cm & ch_roa haben eine Korrelation von 0. Also erklärn sie die Vorhersage "perfekt nicht".
-> spielt keine Rolle. 
-> By the way: Hier wäre ein guter Ansatz zur Feature-Selection




FAZIT: Bisherige Vorhersage bleibt eine Katastrophe. Wir brauchen ein komplexeres Netz.Rechenzeit geht aber durch die Decke ( mehrere Tage).

Bisherige Lösungsansätze: 
  -> Komplett verworfen. 6h an einer Lösung gearbeitet, die in R funktioniert. Geschditert. Wir machen das mit Sebbi's Netz oder auf 
    Google Colab, also in Python. Alles andere (GPU-Nutzung in R, ...) ist keine Option mehr.

Weitere TODOs: 
  -> [DONE] LogReg mit allen Daten basteln und versuchen, auf vernünftige Ergebnisse zu kommen. Baseline muss stehen.
  -> [DONE] Funktion "evaluate" basteln, die mir die immer gleiche Tabelle ausgibt: Acc, Error, Prec, Sens, F1, F2
  -> Irgendwas stimmt mit dem F2-Score nicht. Wahrscheinlich sind in der die Gewichtungen von Prec und Sens durcheinander gekommen. Oder        ich habs falsch verstanden und wir wollen in Wirklichkeit den F0.5-Score. 
  -> 
  -> PDF-Output muss für uns angepasst werden:
      -> HHU-Beschriftungen- und Bilder
      -> Unsere Namen
      -> Unsere Quellen & deren korrekte Zitation



Ich habe eben bei Bao egschaut. Unser Baseline modell hat eine prec von ca 45%. Bao war mit absolut allen Modellen DEUTLICH darunter (1-10%). Sind wir die Besten? Nein. Irgendwas ist hier falsch. Morgen nochmal schauen, wie sich LOGIT verhält, wenn sie nur Rohdaten bekommt (normalisiert und upsampled natürlich). Alternativ kann man sich zu dem Gedanken hinreißen lassen, dass Bao und co einfach nicht gut gearbeitet haben...mal schauen. 


