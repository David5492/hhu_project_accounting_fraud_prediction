---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
documentclass: book
bibliography: references.bib
---

# Methoden {#methods} 

Diese Arbeit vergleicht die Vorhersagequalität der in der Accounting Fraud Detection gängigen logistischen Regression nach Dechow et al. (vgl. Bao et al. 2020, S. 2) mit der von neuronalen Netzen.
Die logistische Regression ist eng mit der linearen Regression verwandt und wird zur binären Schätzung einer Klassenzugehörigkeit verwandt (vgl. Géron 2019, S. 144). Dabei berechnet sie eine gewichtete Summe von Inputfaktoren und aggregiert sie zu einer Wahrscheinlichkeit zwischen 0 und 1. Liegt diese Wahrscheinlichkeit bei mindestens 0.5, so wird die Klasse „1“ vorhergesagt, welcher in dieser Arbeit der Klasse „fraud“ entspricht (vgl. Géron 2019, S. 144). Hierzu wird für jedes Attribut einer Beobachtung eine Sigmoid-Funktion verwendet, welche S-förmig vom Minimum bis zum Maximum des jeweiligen Attributs verläuft und die Verteilungen der Merkmalsausprägungen möglichst gut nach Klassenzugehörigkeit abgrenzt. Je weiter eine Merkmalsausprägung von dieser Grenze entfernt ist, desto näher ist der Funktionswert an der 1 oder der 0 (vgl. Géron 2019, S. 148).
Neuronale Netze bestehen aus drei Sorten von Schichten: Input-Schichten, welche Daten einlesen, versteckte Schichten, welche die Daten verarbeiten und Output-Schichten, welche aus den verarbeiteten Daten eine Prognose ableiten (vgl. Géron 2019, S. 286). In dieser Arbeit besteht die Output-Schicht aus lediglich einem Knoten, welche die Klassen „fraud“ abbildet. Die Anzahl der Knoten der Input-Schicht entspricht der Anzahl der Variablen im Datensatz. Die Knoten einer Schicht sind jeweils mit jedem Knoten seiner nachfolgenden Schicht durch „Gewichte“ verbunden. Jeder einzelne Knoten aggregiert die Signale, die er empfängt über deren Gewichte zu einer Zahl und wendet eine Aktivierungsfunktion an (vgl. Géron 2019, S. 282). Übersteigt der Funktionswert einen gegeben Schwellenwert, so „feuert“ das Neuron, was bedeutet, dass es ein Signal größer 0 an die Neuronen der nächsten Schicht weitergibt (vgl. Géron 2019, S. 282-283). Die Gewichte und alle Schwellenwerte werden durch Backpropagation unter Zuhilfenahme des Gradient Descent Algorithmus verbessert (vgl. Géron 2019, S.119 und S, 286). Sind alle Trainingsdaten einmal zum Training herangezogen worden, bedeutet das, dass das Netz für „eine Epoche“ trainiert wurde (vgl. Géron 2019, S. 127). In dieser Arbeit wird ein Netz über 100 Epochen hinweg trainiert.


 AB HIER HANDELT SICH DER AUFSCHRIEB MEHR UM NOTIZEN ALS UM EINE ABGABEFÄHIGE VERSION.
 
 
 
```{r, include=FALSE}
# install.packages("stringi", type="binary")
# install.packages('caret')
# install.packages('neuralnet')
# install.packages('dplyr')
# install.packages('Hmisc')
# install.packages('smotefamily')
# install.packages('readr')
# install.packages('rio', type='binary')
# install.packages('bookdown', type='binary')

```
```{r}

library(neuralnet)
library(caret)
library(dplyr)
library(Hmisc)
library(smotefamily)
library(readr)
library(rio)
```
 
 
```{r}
set.seed = 42

all_data <- import("data/uscecchini28.csv")
dim(all_data)
```
Wir haben 146045 Beobachtungen auf 51 Variablen. Wir müssen die fehlenden Werte Ientifizieren und die Spalten, welche zu viele davon enthalten löschen. Danach löschen wir die übrigen Zeilen, welche NaNs enthalten. Dies dient dem Umstand, dass NN mit NaN-Werten nicht arbeiten können. Wir verwenden für LogReg und NN den glichen Datensatz, um die Performance der Modelle vergleichen zu können. 

Um die Spalten zu identifizieren, die viele Na-Werte beinhalten, wird die folgende Funktion egschrieben:
```{r}
check_na <- function(data){
  
  nan_number_list <- vector()
  nan_col_list <- vector()
  
  for (col in colnames(data)){
    nan_number <- sum(is.na(data[col]))
    nan_number_list <- append(nan_number_list, nan_number, after = length(nan_number_list))
    nan_col_list <- append(nan_col_list, col, after = length(nan_col_list))
    nan_tabel <- as.data.frame(cbind(nan_number_list, nan_col_list))
    new_nan_tabel <- nan_tabel[order(- nan_number_list),]
    rownames(new_nan_tabel) <- NULL
  }
  
  print(head(new_nan_tabel, 24))
}

check_na(all_data)
```
3 Sepparate Datensätze und die Rohdaten und Ratios auf NaN checken. 

```{r}

ratio_names <- c("fyear", "misstate", "dch_wc", "ch_rsst", "dch_rec", "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")
ratio_data <- all_data[, match(ratio_names, names(all_data))]

raw_data <- all_data[, -match(ratio_names, names(all_data))]
```

```{r}
check_na(ratio_data)
```

```{r}
ratio_data <- ratio_data[complete.cases(ratio_data),]
```
ratio_data enthält jetzt alle Zeilen der Ration-Daten ohne NaN-Einträge. Es bleiben 126483 auf 14 Variablen übrig.

```{r}
check_na(raw_data)
```

```{r}

raw_data <- raw_data[, -match(c("p_aaer", "new_p_aaer"), names(raw_data))]
raw_data <- raw_data[complete.cases(raw_data),]
```
raw_data enthält jetzt alle Zeilen der Rohdaten ohne NaN-Einträge. Es bleiben 132516 auf 35 Variablen übrig.
"p_aaer" & "new_p_aaer" bestanden fast ausschließlich aus NaN-Werten, weshalb die Spalten komplett gelöscht wurden. 

HIER STIMMT WAS NICHT: 35 Vars? Das sollten 26 (vormals 28) sein. Checken was da los ist. 
```{r}
raw_names <- c("fyear", "misstate", "act", "ap", "at", "ceq", "che", "cogs", "csho", "dlc", "dltis", "dltt", "dp", "ib", "invt", "ivao", "ivst", "lct", "lt", "ni", "ppegt", "pstk", "re", "rect", "sale", "sstk", "txp", "txt", "xint", "prcc_f")

raw_data <- all_data[, match(raw_names, names(all_data))]

raw_and_ratio_names <- c(raw_names, ratio_names)

strange_rest_data <- all_data[, -match(raw_and_ratio_names, names(all_data))]

print(names(strange_rest_data))
```
fyear und misstate sind nützlich. p_aaer und new_p_aaer sind eh schon raus. Der Rest kommt auch weg, weil nutzlos. fyear wird zur Normalisierung der 2 Datensätze raw_data und ratio_data verwendet und misstate ist die Zielvariable.

Es fliegen insgesamt raus: 
  -> p_aaer
  -> new_p_aaer
  -> gvkey
  -> sich (damit verbundene Annahme: Industriezweig spielt keine Rolle)
  -> insbnk
  -> understatement
  -> option
  
Übrig bleiben: 
  -> 126483 fälle auf 14 ratios-Variablen
  -> 146045 fälle auf 28 Rohdaten-Variablen
  -> 1 Zielvariable ("misstate")
  -> 1 Variable zur Normalisierung "fyear"

(an der Stelle bin ich zurück nach oben gegangen und habe fyear und misstate zu den beiden Datensätzen hinzugefügt)
```{r}

```
  


TEST: Es gibt mehr Roh-Fälle als Ratio-Fälle. Kann ich die überschüssigen Rohfälle einfach löschen oder sind in denen proportional mehr oder weniger Betrugsfälle?

```{r}
raw_frac <- sum(raw_data$misstate) / nrow(raw_data)
ratio_frac <- sum(ratio_data$misstate) / nrow(ratio_data)

ratio_frac / raw_frac

```
Jaaaaa, dumm gelaufen. In den ratio-Daten ist die Dichte an Betrugsfällen knapp 9% höher. Dem kann man nur auf eine Weise begegnen: Die Rohdaten und Ratio-Daten müssen die gleichen Beobachtungen beinhalten, da man die auf ihnen beruhenden Ergebnisse später sonst nicht vergleichen kann.

Also muss der ganze Cleaning-Prozess nochmal durchgeführt werden, aber in der Reichenfolge:

  1. Daten als "data" Laden
  2. Spalten "p_aaer" und "new_p_aaer" löschen
  3. Alle Zeilen mit NaN-Werten löschen
  4. raw_data, ratio_data und all_data bilden
  5. deskriptive Statistiken können mit den Datensätzen         aus 4. angefertigt werden 
  6. Die Datensätze aus 4. via Jahreszahl normalisieren
  
Dann ist die Datenvorbereitung fertig und man kann Modelle damit rechnen. Bis zu dem Punkt geht die nächste Zelle:

```{r}
## 1.
data <- import("data/uscecchini28.csv")


## 2.
data <- data[,-match(c("p_aaer", "new_p_aaer"), names(data))]


## 3.
data <- data[complete.cases(data),]


## 4.
raw_names <- c("fyear", "misstate", "act", "ap", "at", "ceq", "che", "cogs", "csho", "dlc", "dltis", "dltt", "dp", "ib", "invt", "ivao", "ivst", "lct", "lt", "ni", "ppegt", "pstk", "re", "rect", "sale", "sstk", "txp", "txt", "xint", "prcc_f")
raw_data <- data[, match(raw_names, names(data))]

ratio_names <- c("fyear", "misstate", "dch_wc", "ch_rsst", "dch_rec", "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")
ratio_data <- data[, match(ratio_names, names(data))]

all_names <- c(raw_names, "dch_wc", "ch_rsst", "dch_rec", "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")
all_data <- data[, match(all_names, names(data))]


## 5. 
## SKIP. Statistiken bilden geht auch noch wann anders. 

## 6. 

normalize <- function(data){
  ausgabe <- matrix(NA, nrow = nrow(data), ncol = ncol(data))
  dimnames(ausgabe)[1] <- list(names(data))
  years <- unique(data$fyear)
  
  for (year in years){
    
    one_year <- data[data$fyear == year, ]
    
    for (col in names(one_year)){
      # data[data$fyear == year, col] <- (one_year[col]- min(one_year[col])) /(max(one_year[col])-min(one_year[col])) 
      sub_frame <- matrix(NA, nrow = nrow(one_year), ncol = ncol(one_year))
      dimnames(sub_frame)[1] <- list(names(one_year))
      sub_frame[col] <- (one_year[col]- min(one_year[col])) /(max(one_year[col])-min(one_year[col]))
      ausgabe <- sub_frame
    }
    
  }
  # data
  ausgabe
}

norm_all_data <- normalize(all_data)

```

```{r}
library(data.table)
test <- setDT(data)[, val_norm := val/val[year==1][1L] , by = group]
```


  
  
  
  
### BREAK HIER. ALLES DAVOR IST VOM 30.6.
Die Variable, "p_aaer" und "new_p_aaer", "ch_cm" und "ch_cs" fallen durch extrem viele NaN-Werte auf und werden komplett entfernt. Die übrigen Variablen bestehen zu maximal ca 10% aus NaN-Werten und werden daher als unbedenklich eingestuft. "new_p_aaer" und alle Beobachtungen, welche NaN-Werte enthalten, werden beseitigt. DAZU NOCH EINE EXTRASPALTE "nan_frac" EINFÜGEN UND nan_col_list ZUM INDEX MACHEN. nan_number_list zu "NaN-Werte".

```{r}
data <- data[, -match(c("new_p_aaer", "p_aaer", "ch_cm", "ch_cs"), names(data))]
data <- data[complete.cases(data),]
dim(data) # Übrige Beobachtungen: 116478 auf 49 Variablen
```
Damit ein NN oder die LogReg Inputdaten verwerten kann, müssen diese zwischen 0 und 1 normiert sein. Und zwar Jahr für Jahr. 
```{r}
normalize <- function(col, na.rm = TRUE) {
    return((col- min(col)) /(max(col)-min(col)))
}
# as.data.frame(apply(df$name, normalize))
for (col in colnames(data))
{
  data[col] <- normalize(data[col])
}

```

trainings- und testdaten generieren (seed = immer noch 42)

```{r}
smp_size <- floor(0.70 * nrow(data))

train_ind <- sample(seq_len(nrow(data)), size = smp_size)

train <- data[train_ind, ]

true_frac <- sum(train[,7] == 1) / (smp_size - sum(train[,7] == 1)) #sagt aus, wie hoch Anteil der Betrugsfälle im Testdatenssatz ist


train_smote_object <- SMOTE(train[, -7], train[,7], K = 5, dup_size = 1 / true_frac)$data
train_smote_object$class <- as.numeric(train_smote_object$class)

test <- data[-train_ind, ]
```



```{r, include=TRUE}
#das sind nur nebenrechnungen

# Anteil der Betrugsfälle im Trainingsdatensatz vor SMOTE: 
sum(train[,7] == 1) / (smp_size - sum(train[,7] == 1))

# Anteil der Betrugsfälle im Trainingsdatensatz nach SMOTE: 
sum(train_smote_object$class == "1") / nrow(train_smote_object)

```

Jetzt werden 3 NN trainiert: Alle Daten, nur Roh, nur Ratios. seed = 42

-> DAS WIRD VON SEBBI GEMACHT


```{r}
# Funktion "evaluate basteln. Rechnet mit y_test und y_pred. 
# Ausgabe ist Tabelle mit Acc, Error, Prec, Sens, F1, F2. 
# Die Metriken werden nur für die True-Zeile (also fraud) berechnet.

# Confusion matrix sieht so aus: Spalten = pred(0,1), Zeilen = actual(0,1)

evaluate <- function(test, pred){
  
  pred <- ifelse(pred > 0.5, 1, 0)
  confusion <- table(test, pred)
  total_acc <- numeric(2)
  total_acc[1] <- NaN
  total_acc[2] <- round((confusion[1,1] + confusion[2,2]) / sum(confusion),4)

  
  prec <- numeric(2)
  prec[1] <- NaN
  prec[2] <- round(confusion[2,2] / (confusion[2,2] + confusion[2,1]),4)
  
  sens <- numeric(2)
  sens[1] <- NaN
  sens[2] <- round(confusion[2,2] / (confusion[2,2] + confusion[1,2]),4)
  
  F1 <- numeric(2)
  F1[1] <- NaN
  F1[2] <- round(2*(prec[2]*sens[2])/(prec[2] + sens[2]), 4)
  
  F2 <- numeric(2)
  F2[1] <- NaN
  F2[2] <- round((1 + 2^2) * (prec[2]*sens[2]) / (2^2 * prec[2] + sens[2]), 4)
  
  print(cbind(confusion, total_acc, prec, sens, F1, F2))
  
}


y_test <- c(1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1)
y_pred <- c(1,0,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1)

evaluate(test=y_test, pred=y_pred ) # sou, Evaluierungssfunktion steht. 
```

Logistische Regression basteln:
```{r}
# trainingsdaten in train_smote_object
# testdaten in test

mylogit <- glm(class ~., data = train_smote_object, family = "binomial")
y_pred_logit <- predict.glm(mylogit, test[, !names(train) %in% c('misstate')])

evaluate(test$misstate, y_pred_logit)
```
Scheinbar ist irgendetwas perfekt in den Traingsdaten korreliert. Herausfinden was

```{r}
round(cor(train_smote_object$class, train_smote_object[, !names(train_smote_object) %in% c('class')]), 2)
```
GEFUNDEN: ch_cm, ch_roa, dpi & ni haben eine Korrelation von 0. 
  -> spielt keine Rolle. 
  -> By the way: Hier wäre ein guter Ansatz zur Feature-Selection

TODO:
  -> PDF-Output muss für uns angepasst werden:
      -> HHU-Beschriftungen- und Bilder
      -> Unsere Namen
      -> Unsere Quellen & deren korrekte Zitation



Ich habe eben bei Bao egschaut. Unser Baseline modell hat eine prec von ca 45%. Bao war mit absolut allen Modellen DEUTLICH darunter (1-10%). Sind wir die Besten? Nein. Irgendwas ist hier falsch. Morgen nochmal schauen, wie sich LOGIT verhält, wenn sie nur Rohdaten bekommt (normalisiert und upsampled natürlich). Alternativ kann man sich zu dem Gedanken hinreißen lassen, dass Bao und co einfach nicht gut gearbeitet haben...mal schauen. 


AUFGABE: 
  -> Normalisierung der Daten innerhalb jeden Jahres
  -> [DONE] LogReg mit allen Daten 
  -> Log Reg mit nur Rohdaten
  -> Log Reg mit nur Ratios





















