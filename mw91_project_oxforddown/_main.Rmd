---
#####################
##  output format  ##
#####################
# The lines below makes the 'knit' button build the entire thesis 
# Edit the line 'thesis_formats <- "pdf"' to the formats you want
# The format options are: 'pdf', 'bs4', 'gitbook', 'word'
# E.g. you can build both pdf and html with 'thesis_formats <- c("pdf", "bs4")'
knit: (function(input, ...) {
    thesis_formats <- "pdf";
    
    source("scripts_and_filters/knit-functions.R");
    knit_thesis(input, thesis_formats, ...)
  })

#####################
## thesis metadata ##
#####################
title: |
  `oxforddown`: \
  An Oxford University Thesis \
  Template for R Markdown
author: Author Name
college: Your College
university: University of Oxford
university-logo: templates/beltcrest.pdf
university-logo-width: 5cm
submitted-text: A thesis submitted for the degree of
degree: Doctor of Philosophy
degreedate: Michaelmas 2018
abstract: |
  `r paste(readLines("front-and-back-matter/_abstract.Rmd"), collapse = '\n  ')`
acknowledgements: |
  `r paste(readLines("front-and-back-matter/_acknowledgements.Rmd"), collapse = '\n  ')`
dedication: For Yihui Xie
abbreviations: |
  `r paste(readLines("front-and-back-matter/_abbreviations.Rmd"), collapse = '\n  ')`

#######################
## bibliography path ##
#######################
bibliography: references.bib

########################
## PDF layout options ###
#########################
### submitting a master's thesis ###
# set masters-submission: true for an alternative, anonymous title page with 
# candidate number and word count
masters-submission: false
candidate-number: 123456
word-count: "10,052"

# if you want to use a different title page altogether, provide a path to a 
# .tex file here and it will override the default Oxford one
# alternative-title-page: templates/alt-title-page-example.tex

### abbreviations ###
abbreviations-width: 3.2cm
abbreviations-heading: List of Abbreviations


### citation and bibliography style ###
bibliography-heading-in-pdf: Works Cited

# biblatex options #
# unless you run into 'biber' error messages, use natbib as it lets you customise your bibliography directly
use-biblatex: true
bib-latex-options: "style=authoryear, sorting=nyt, backend=biber, maxcitenames=2, useprefix, doi=true, isbn=false, uniquename=false" #for science, you might want style=numeric-comp, sorting=none for numerical in-text citation with references in order of appearance

# natbib options #
# natbib runs into fewer errors than biblatex, but to customise your bibliography you need to fiddle with .bst files
use-natbib: false # to use natbib, set this to true, and change "output:bookdown::pdf_book:citation_package:" to "natbib"
natbib-citation-style: authoryear #for science, you might want numbers,square
natbib-bibliography-style: templates/ACM-Reference-Format.bst #e.g. "plainnat", unsrtnat, or path to a .bst file

### correction highlighting ###
corrections: true

### link highlighting ###
colored-not-bordered-links: true # true = highlight text of links - false = highlight links with border

# Set the link text/bord coloring here, in RGB. 
# Comment out a variable to just use whatever the text's existing color is. 
# If you wish NOT to highlight links, set colored-not-bordered-links: true, 
# and comment out the colors below
urlcolor-rgb: "0,0,139"
citecolor-rgb: "0,33,71"
# linkcolor-rgb: "0,33,71"  # coloring normal links looks a bit excessive, as it highlights also all links in the table of contents


### binding / margins ###
page-layout: nobind #'nobind' for equal margins (PDF output), 'twoside' for two-sided binding (mirror margins and blank pages), leave blank for one-sided binding (left margin > right margin)

### position of page numbers ###
ordinary-page-number-foot-or-head: foot #'foot' puts page number in footer, 'head' in header
ordinary-page-number-position: C  #C = center, R = right, L = left. If page layout is 'twoside', O = odd pages and E = even pages. E.g. RO,LE puts the page number to the right on odd pages and left on even pages
chapter-page-number-foot-or-head: foot #you may want it to be different on the chapter pages
chapter-page-number-position: C

### position of running header ###
running-header: true #indicate current chapter/section in header?
running-header-foot-or-head: head
running-header-position-leftmark: LO #marks the chapter. If layout is 'nobind', only this is used.
running-header-position-rightmark: RE  #marks the section.


### draft mark ###
draft-mark: false # add a DRAFT mark?
draft-mark-foot-or-head: foot ##'foot' = in footer, 'head' = in header
draft-mark-position: C

### section numbering ###
section-numbering-depth: 2 # to which depth should headings be numbered?

### tables of content ###
toc-depth: 1 # to which depth should headings be included in table of contents?
lof: true # include list of figures in front matter?
lot: true # include list of tables in front matter?
mini-toc: true  # include mini-table of contents at start of each chapter? (this just prepares it; you must also add \minitoc after the chapter titles)
mini-lot: false  # include mini-list of tables by start of each chapter?
mini-lof: false  # include mini-list of figures by start of each chapter?

### code block spacing ###
space-before-code-block: 10pt
space-after-code-block: 8pt

### linespacing ###
linespacing: 22pt plus2pt # 22pt is official for submission & library copies
frontmatter-linespacing: 17pt plus1pt minus1pt #spacing in roman-numbered pages (acknowledgments, table of contents, etc.)

### other stuff ###
abstractseparate: false  # include front page w/ abstract for examination schools?
includeline-num: false #show line numbering in PDF?


#####################
## output details  ##
#####################
output:
  bookdown::pdf_book:
    citation_package: biblatex
    template: templates/template.tex
    keep_tex: true
    pandoc_args: "--lua-filter=scripts_and_filters/colour_and_highlight.lua"
  bookdown::bs4_book: 
    css: 
      - templates/bs4_style.css
      - templates/corrections.css # remove to stop highlighting corrections
    theme:
      primary: "#6D1919"
    repo: https://github.com/ulyngs/oxforddown
    pandoc_args: "--lua-filter=scripts_and_filters/colour_and_highlight.lua"
  bookdown::gitbook:
    css: templates/style.css
    config:
      sharing:
        facebook: false
        twitter: yes
        all: false
  bookdown::word_document2:
    toc: true   
link-citations: true
documentclass: book
always_allow_html: true #this allows html stuff in word (.docx) output
---


```{r install_packages, include=FALSE}
source('scripts_and_filters/install_packages_if_missing.R')
```

```{r create_chunk_options, include=FALSE, eval=knitr::is_latex_output()}
source('scripts_and_filters/create_chunk_options.R')
source('scripts_and_filters/wrap_lines.R')
```

<!--
Include the create_chunk_options chunk above at the top of your index.Rmd file
This will include code to create additional chunk options (e.g. for adding author references to savequotes)
and to make sure lines in code soft wrap
If you need to create your own additional chunk options, edit the file scripts/create_chunk_options.R
-->

<!-- This chunk includes the front page content in HTML output -->
```{r ebook-welcome, child = 'front-and-back-matter/_welcome-ebook.Rmd', eval=knitr::is_html_output()}
```

<!--chapter:end:index.Rmd-->

---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
#bibliography: references.bib
---

# Einleitung {#intro}


Vergangene und gegenwärtige Bilanzskandale wie ENRON, Worldcom oder Wirecard führen immer wieder zu Diskussionen über die Vertrauenswürdigkeit des Kapitalmarkts sowie über die Vertrauenswürdigkeit der Abschlussprüfung. Bilanzskandale führen weit-reichende Konsequenten mit sich welche eine genauere Analyse des Warums und wie anstoßen (vgl. Boecker/Zwirner 2012, S. 1).
Diese Arbeit hat das Ziel, den Einsatz neuronaler Netze gegenüber der gängigeren Me-thode der logistischen Regression (vgl. Bao et al., 2020) zu vergleichen und dabei her-auszufinden, ob erstere signifikant besser darin sind, so viele Betrugsfälle wie möglich zu identifizieren, ohne die triviale Annahme zu treffen, dass jeder Fall ein Betrugsfall ist. Bei der Modellevaluation wird angenommen, dass ein nicht-identifizierter Betrugsfall doppelt so schwer wiegt, wie ein prognostizierter Betrugsfall, der sich als Nicht-Betrug herausstellt.
Die Metrik zur Messung der Ergebnisse ist der $F_{\beta}$-Score (vgl. Tharwat 2020, S. 174). Dieser bildet das harmonische Mittel aus Präzision und Sensitivität. Die Präzision sagt aus, wie viele vorhergesagte Betrugsfälle tatsächlich Betrugsfälle sind, wobei die Sensi-tivität dabei auf die Frage antwortet, wie viele der tatsächlichen Betrugsfälle als solche identifiziert wurden. Weil die Sensitivität im Kontext der Fraud Detection bedeutungs-voller erscheint, wird sie in dieser Arbeit doppelt so hoch gewichtet wie die Präzision. Die Formel dieser Metrik lautet dabei:
$$F_{\beta = 2} = (1+\beta^{2})*\frac{P*R}{\beta * P*R}$$

<!--chapter:end:01-introduction.Rmd-->

---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: references.bib
---


# Accounting Fraud und Machine Learning {#AccFraud_and_ML}

Das Handelsgesetzbuch (HGB) sieht gemäß § 317 Absatz (Abs.) 1 Satz (S.) 3 vor, dass der Abschlussprüfer im Rahmen der Abschlussprüfung, Unrichtigkeiten und Verstöße welche der Ordnungsmäßigkeit des Abschlusses entgegen stehen, erkennt und entsprechend deklariert. Was unter den Begrifflichkeiten Unrichtigkeit und Verstöße zu verstehen ist, wird durch den Gesetzgeber an dieser Stelle nicht weiter konkretisiert (vgl. Zwernemann et al. 2015, S. 22; § 317 Abs.1 HGB).

Dem entgegen versucht das Institut der Wirtschaftsprüfer (IDW) mit dem veröffentlichten Prüfungsstandard 210 (IDW PS 210) Rechnung zu tragen. Diesem zu entnehmen ist, dass ein Fehlerhafter Abschluss entweder auf Fraud (Verstoß) oder Error (Unrichtigkeit) zurückzuführen ist. Unter dem Begriff Unrichtigkeit wird eine unabsichtliche Angabe im Abschluss verstanden. Konkret bedeutet dies begangene Rechenfehler, eine unbewusst falsche Anwendung von Rechnungslegungsgrundsätzen sowie die falsche Einschätzung von Sachverhalten (vgl. Hlavica et al. 2016, S. 209f.). Der Begriff Verstoß dagegen umfasst eine beabsichtigte Handlung mit dem Ziel rechtswidrige Vorteile zu realisieren. Diese Handlungen konkretisiert der IDW PS 210 als Vermögensschädigungen, Täuschungen und Gesetzesverstöße, welche eine Auswirkung auf die Rechnungslegung zur Folge haben (vgl. Zwernemann et al. 2015, S. 8).
Um die Gründe für eine betrügerische Handlung nachvollziehen zu können, entwickelte Donald Cressey in den 1940er Jahren das sogenannte „Fraud-Triangle“. Dieses Dreieck wird ferner dem IDW PS 210 zugrunde gelegt (vgl. Boecker/Zwirner 2012, S. 2f.). Demnach tritt ein Verstoße dann auf, wenn drei Gegebenheiten als erfüllt angesehen werden können. So muss der Täter eine Gelegenheit zu der Tat haben und einen Anreiz (Motivation) für die Tat verspüren. Als letztes muss der Täter die Tat als moralisch akzeptabel rechtfertigen vor sich selbst rechtfertigen (vgl. Schuchter/Levi 2016, S. 3f.).
Aber nicht nur durch psychologische Ansätze versucht die Wissenschaft Verstöße einzuordnen und zu identifizieren, sondern auch durch eine Vielzahl an Machine Learning Ansätzen, welche Verstöße mittels Algorithmen identifizieren sollen.
Vor dem Hintergrund der potenziellen Gefahren des Bilanzbetrugs werden Letztere zunehmend für die Vorhersage und Aufdeckung von diskretionärer Bilanzpolitik eingesetzt. Die Benchmark in diesem Bereich ist das Dechow et al. Modell, welches auf Grundlage von Accounting and Auditing Enforcement Releases (AAERs) der U.S. Securities and Exchange Comission (SEC) mit Hilfe einer logistischen Regression die Wahrscheinlichkeiten von (bewusst) fehlerhaften Darstellungen schätzt und klassifiziert (Dechow et al. 2011). Hierbei gelten die AAERs als ProxyVariable für die Manipulation der Bilanz. Durch das Voraussetzen der Untersuchungshandlungen seitens der SEC ergibt sich der Vorteil, dass der Typ I Fehler – das Modell sagt fälschlicherweise ein misstatement voraus – deutlich geringer ausfällt. Durch einige wenige Transformationen der logistischen Funktion kann der Einfluss einer jeden unabhängigen Variable durch den entsprechenden Regressionskoeffizienten hinsichtlich der Effektgröße verglichen werden, weswegen die Ergebnisse gut interpretierbar sind. Aus dem Dechow et al. Modell folgt eine korrekte Klassifizierung von misstatements und nonmisstatements von ungefähr 63% (Dechow et al. 2011, S.59). Die Sensitivität, d.h. in wie vielen Fällen das Modell einen misstatement richtig vorhergesagt hat, liegt bei etwas mehr als 68% (339 von 494). Der Typ II Fehler (das Modell klassifiziert ein misstatement als nonmisstatement), der im Rahmen des accounting frauds schwerwiegender ist als der Typ I Fehler (vgl. Lin et al. 2015, S. 468), liegt bei etwas mehr als 31% (155 von 494).
Ein Vergleich der Performance der logistischen Regression mit den neuronalen Netzen, einer weiteren Methode zur Vorhersage von Bilanzbetrug, findet sich in dem Paper von Lin et al. (2015). Aus diesem geht hervor, dass die neuronalen Netze hinsichtlich der Aufdeckung von accounting fraud bessere Ergebnisse liefern als die logistische Regression. Die artificial neural networks (ANNs) erreichen bei dem Testdatensatz eine Genauigkeit von fast 93%. Die Sensitivität von fast 83% liegt zudem deutlich höher als bei der logistischen Regression, bei der 72% der misstatements richtig vorhergesagt wurden (vgl. Lin et al. 2015, S. 465f.).
Die Interpretierbarkeit und verhältnismäßig einfache Anwendbarkeit haben die logistische Regression zu einem beliebten Instrument gemacht, die Ergebnisse hinsichtlich der Vorhersage von Bilanzbetrug werden allerdings von anderen Modellen übertroffen (vgl. Dutta et al. 2017, S. 375). Im Falle der neuronalen Netze ergibt sich wiederum der Nachteil der geringeren Transparenz hinsichtlich der Arbeitsweise des Algorithmus (vgl. Bao et al. 2020, S. 228). Schlussendlich ergibt sich ein Trade-Off zwischen der Interpretierbarkeit und Vorhersagekraft. Nachfolgend liegt der Fokus dieser Ausarbeitung auf der reinen Performance respektive Vorhersagekraft des Modells. Ein Modell, welches misstatements richtig vorhersagt, erscheint bei der Klassifizierung von Bilanzbetrug wichtiger als ein Modell, aus dem abgelesen werden kann, welche Variablen den misstatement wie stark beeinflussen. An dieser Stelle sollte erwähnt werden, dass auch ein Modell, welches in 99% der Fälle die richtige Vorhersage trifft, hinsichtlich der Aufdeckung von accounting fraud nicht geeignet sein muss. Durch die Problematik der signifikanten sample imbalance – Betrugsfälle sind stark unterrepräsentiert – ist es möglich, dass misstatements durch das Modell nicht erkannt respektive falsch klassifiziert werden, also Typ II Fehler auftreten können.

<!--chapter:end:02-AccFraud_and_ML.Rmd-->

---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
bib-humanities: true
documentclass: book
bibliography: references.bib
---

# Datensatz {#dataset} 
Ein besonderer Fokus liegt aus diesem Grund auf der Minimierung des Typ II Fehlers respektive der Maximierung der Sensitivität des Modells. Eine höchstmögliche Genauigkeit ist gut, aufgrund der sehr geringen Anzahl an bilanziellen Verfehlungen verglichen zu der Stichprobengröße sollte hier allerdings kein Schwerpunkt liegen.
Zum Training und Test des Machine Learning Modells, wird ein Datensatz aus der Veröffentlichung von Bao et al. (2020) verwendet. Die Autoren haben diesen im Internet auf der Seite „GitHub“ zur Verfügung gestellt (vgl. Bao et al. 2020, GitHub Repository). Dieser besteht aus allen öffentlich gelisteten US-amerikanischen Firmen im Zeitraum von 1991 bis 2008. Die Accounting-Betrugsfälle aus den „Accounting and Auditing Enforcement Releases“ (AAER), die von der United States Securities and Exchange Commission (SEC) im gleichen Zeitraum veröffentlicht worden sind (vgl. Bao et al. 2020, S. 207). Der Datensatz listet für jeden Eintrag 28 verschiedene finanzielle Items auf. Diese setzen sich aus den Veröffentlichungen von Cecchini et al. (2010) und Dechow et al. (2011) zusammen. Die finanziellen Items stammen aus vier verschiedenen Bereichen, der Bilanz (z.B. gesamte Forderungen), der Gewinn- und Verlustrechnung (z.B. Nettoumsatz), der Kapitalflussrechnung (z.B. Langzeitemission von Schuldtiteln) und dem Marktwert (z.B. Common Shares Outstanding).
Da Accounting-Betrugsfälle eher seltener vorkommen (vgl. Dutta et al. 2011, S. 381), weist der Datensatz eine große Verteilungsungleichheit zwischen den Betrugs- und Nicht-betrugsfällen auf. Weniger als ein Prozent aller Einträge im Datensatz sind hierbei Betrugsfälle. Weitere Betrugsfälle könnten über andere Datenbanken gesucht werden, hierbei gibt es allerdings das Problem, dass die Betrugsfälle identifiziert und einzeln herausgesucht werden müssen. Daher besteht hier ein „class imbalance“ Problem, das mithilfe des Machine Learning Algorithmus gelöst werden muss.
Zudem sind nur Betrugsfälle bis zum Jahr 2008 in diesem Datensatz erhalten, da die SEC nach der Finanzkrise ihre Prioritäten geändert hat (vgl. Bao et al. 2020, S. 208). Sollte sich die Art und Weise mit der Accounting-Betrug durchgeführt wird in den Jahren danach geändert haben, so können diese Fälle unter Umständen nicht vom Algorithmus erkannt werden.

<!--chapter:end:03-dataset.Rmd-->

---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
documentclass: book
bibliography: references.bib
---

# Methoden {#methods} 

Diese Arbeit vergleicht die Vorhersagequalität der in der Accounting Fraud Detection gängigen logistischen Regression nach Dechow et al. (vgl. Bao et al. 2020, S. 2) mit der von neuronalen Netzen.
Die logistische Regression ist eng mit der linearen Regression verwandt und wird zur binären Schätzung einer Klassenzugehörigkeit verwandt (vgl. Géron 2019, S. 144). Dabei berechnet sie eine gewichtete Summe von Inputfaktoren und aggregiert sie zu einer Wahrscheinlichkeit zwischen 0 und 1. Liegt diese Wahrscheinlichkeit bei mindestens 0.5, so wird die Klasse „1“ vorhergesagt, welcher in dieser Arbeit der Klasse „fraud“ entspricht (vgl. Géron 2019, S. 144). Hierzu wird für jedes Attribut einer Beobachtung eine Sigmoid-Funktion verwendet, welche S-förmig vom Minimum bis zum Maximum des jeweiligen Attributs verläuft und die Verteilungen der Merkmalsausprägungen möglichst gut nach Klassenzugehörigkeit abgrenzt. Je weiter eine Merkmalsausprägung von dieser Grenze entfernt ist, desto näher ist der Funktionswert an der 1 oder der 0 (vgl. Géron 2019, S. 148).
Neuronale Netze bestehen aus drei Sorten von Schichten: Input-Schichten, welche Daten einlesen, versteckte Schichten, welche die Daten verarbeiten und Output-Schichten, welche aus den verarbeiteten Daten eine Prognose ableiten (vgl. Géron 2019, S. 286). In dieser Arbeit besteht die Output-Schicht aus lediglich einem Knoten, welche die Klassen „fraud“ abbildet. Die Anzahl der Knoten der Input-Schicht entspricht der Anzahl der Variablen im Datensatz. Die Knoten einer Schicht sind jeweils mit jedem Knoten seiner nachfolgenden Schicht durch „Gewichte“ verbunden. Jeder einzelne Knoten aggregiert die Signale, die er empfängt über deren Gewichte zu einer Zahl und wendet eine Aktivierungsfunktion an (vgl. Géron 2019, S. 282). Übersteigt der Funktionswert einen gegeben Schwellenwert, so „feuert“ das Neuron, was bedeutet, dass es ein Signal größer 0 an die Neuronen der nächsten Schicht weitergibt (vgl. Géron 2019, S. 282-283). Die Gewichte und alle Schwellenwerte werden durch Backpropagation unter Zuhilfenahme des Gradient Descent Algorithmus verbessert (vgl. Géron 2019, S.119 und S, 286). Sind alle Trainingsdaten einmal zum Training herangezogen worden, bedeutet das, dass das Netz für „eine Epoche“ trainiert wurde (vgl. Géron 2019, S. 127). In dieser Arbeit wird ein Netz über 100 Epochen hinweg trainiert.


 AB HIER HANDELT SICH DER AUFSCHRIEB MEHR UM NOTIZEN ALS UM EINE ABGABEFÄHIGE VERSION.
 
 
 
```{r, include=FALSE}
# install.packages("stringi", type="binary")
# install.packages('caret')
# install.packages('neuralnet')
# install.packages('dplyr')
# install.packages('Hmisc')
# install.packages('smotefamily')
# install.packages('readr')
# install.packages('rio', type='binary')
# install.packages('bookdown', type='binary')

```
```{r}

library(neuralnet)
library(caret)
library(dplyr)
library(Hmisc)
library(smotefamily)
library(readr)
library(rio)
```
 
 
```{r}
set.seed = 42

all_data <- import("data/uscecchini28.csv")
dim(all_data)
```
Wir haben 146045 Beobachtungen auf 51 Variablen. Wir müssen die fehlenden Werte Ientifizieren und die Spalten, welche zu viele davon enthalten löschen. Danach löschen wir die übrigen Zeilen, welche NaNs enthalten. Dies dient dem Umstand, dass NN mit NaN-Werten nicht arbeiten können. Wir verwenden für LogReg und NN den glichen Datensatz, um die Performance der Modelle vergleichen zu können. 

Um die Spalten zu identifizieren, die viele Na-Werte beinhalten, wird die folgende Funktion egschrieben:
```{r}
check_na <- function(data){
  
  nan_number_list <- vector()
  nan_col_list <- vector()
  
  for (col in colnames(data)){
    nan_number <- sum(is.na(data[col]))
    nan_number_list <- append(nan_number_list, nan_number, after = length(nan_number_list))
    nan_col_list <- append(nan_col_list, col, after = length(nan_col_list))
    nan_tabel <- as.data.frame(cbind(nan_number_list, nan_col_list))
    new_nan_tabel <- nan_tabel[order(- nan_number_list),]
    rownames(new_nan_tabel) <- NULL
  }
  
  print(head(new_nan_tabel, 24))
}

check_na(all_data)
```
3 Sepparate Datensätze und die Rohdaten und Ratios auf NaN checken. 

```{r}

ratio_names <- c("fyear", "misstate", "dch_wc", "ch_rsst", "dch_rec", "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")
ratio_data <- all_data[, match(ratio_names, names(all_data))]

raw_data <- all_data[, -match(ratio_names, names(all_data))]
```

```{r}
check_na(ratio_data)
```

```{r}
ratio_data <- ratio_data[complete.cases(ratio_data),]
```
ratio_data enthält jetzt alle Zeilen der Ration-Daten ohne NaN-Einträge. Es bleiben 126483 auf 14 Variablen übrig.

```{r}
check_na(raw_data)
```

```{r}

raw_data <- raw_data[, -match(c("p_aaer", "new_p_aaer"), names(raw_data))]
raw_data <- raw_data[complete.cases(raw_data),]
```
raw_data enthält jetzt alle Zeilen der Rohdaten ohne NaN-Einträge. Es bleiben 132516 auf 35 Variablen übrig.
"p_aaer" & "new_p_aaer" bestanden fast ausschließlich aus NaN-Werten, weshalb die Spalten komplett gelöscht wurden. 

HIER STIMMT WAS NICHT: 35 Vars? Das sollten 26 (vormals 28) sein. Checken was da los ist. 
```{r}
raw_names <- c("fyear", "misstate", "act", "ap", "at", "ceq", "che", "cogs", "csho", "dlc", "dltis", "dltt", "dp", "ib", "invt", "ivao", "ivst", "lct", "lt", "ni", "ppegt", "pstk", "re", "rect", "sale", "sstk", "txp", "txt", "xint", "prcc_f")

raw_data <- all_data[, match(raw_names, names(all_data))]

raw_and_ratio_names <- c(raw_names, ratio_names)

strange_rest_data <- all_data[, -match(raw_and_ratio_names, names(all_data))]

print(names(strange_rest_data))
```
fyear und misstate sind nützlich. p_aaer und new_p_aaer sind eh schon raus. Der Rest kommt auch weg, weil nutzlos. fyear wird zur Normalisierung der 2 Datensätze raw_data und ratio_data verwendet und misstate ist die Zielvariable.

Es fliegen insgesamt raus: 
  -> p_aaer
  -> new_p_aaer
  -> gvkey
  -> sich (damit verbundene Annahme: Industriezweig spielt keine Rolle)
  -> insbnk
  -> understatement
  -> option
  
Übrig bleiben: 
  -> 126483 fälle auf 14 ratios-Variablen
  -> 146045 fälle auf 28 Rohdaten-Variablen
  -> 1 Zielvariable ("misstate")
  -> 1 Variable zur Normalisierung "fyear"

(an der Stelle bin ich zurück nach oben gegangen und habe fyear und misstate zu den beiden Datensätzen hinzugefügt)
```{r}

```
  


TEST: Es gibt mehr Roh-Fälle als Ratio-Fälle. Kann ich die überschüssigen Rohfälle einfach löschen oder sind in denen proportional mehr oder weniger Betrugsfälle?

```{r}
raw_frac <- sum(raw_data$misstate) / nrow(raw_data)
ratio_frac <- sum(ratio_data$misstate) / nrow(ratio_data)

ratio_frac / raw_frac

```
Jaaaaa, dumm gelaufen. In den ratio-Daten ist die Dichte an Betrugsfällen knapp 9% höher. Dem kann man nur auf eine Weise begegnen: Die Rohdaten und Ratio-Daten müssen die gleichen Beobachtungen beinhalten, da man die auf ihnen beruhenden Ergebnisse später sonst nicht vergleichen kann.

Also muss der ganze Cleaning-Prozess nochmal durchgeführt werden, aber in der Reichenfolge:

  1. Daten als "data" Laden
  2. Spalten "p_aaer" und "new_p_aaer" löschen
  3. Alle Zeilen mit NaN-Werten löschen
  4. raw_data, ratio_data und all_data bilden
  5. deskriptive Statistiken können mit den Datensätzen         aus 4. angefertigt werden 
  6. Die Datensätze aus 4. via Jahreszahl normalisieren
  
Dann ist die Datenvorbereitung fertig und man kann Modelle damit rechnen. Bis zu dem Punkt geht die nächste Zelle:

```{r}
## 1.
data <- import("data/uscecchini28.csv")


## 2.
data <- data[,-match(c("p_aaer", "new_p_aaer"), names(data))]


## 3.
data <- data[complete.cases(data),]


## 4.
raw_names <- c("fyear", "misstate", "act", "ap", "at", "ceq", "che", "cogs", "csho", "dlc", "dltis", "dltt", "dp", "ib", "invt", "ivao", "ivst", "lct", "lt", "ni", "ppegt", "pstk", "re", "rect", "sale", "sstk", "txp", "txt", "xint", "prcc_f")
raw_data <- data[, match(raw_names, names(data))]

ratio_names <- c("fyear", "misstate", "dch_wc", "ch_rsst", "dch_rec", "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")
ratio_data <- data[, match(ratio_names, names(data))]

all_names <- c(raw_names, "dch_wc", "ch_rsst", "dch_rec", "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")
all_data <- data[, match(all_names, names(data))]


## 5. 
## SKIP. Statistiken bilden geht auch noch wann anders. 

## 6. 

normalize <- function(data){
  ausgabe <- matrix(NA, nrow = nrow(data), ncol = ncol(data))
  years <- unique(data$fyear)
  # Ausgabe <- matrix(NA, nrow = nrow(data), ncol = ncol(data))
  
  for (year in years){
    
    one_year <- data[data$fyear == year, ]
    
    for (col in names(one_year)){
      data[data$fyear == year, col] <- (one_year[col]- min(one_year[col])) /(max(one_year[col])-min(one_year[col])) # hä. im Test geht das. 
    }
    
  }
  data
}

# norm_all_data <- normalize(all_data)

```

  
  
  
  
### BREAK HIER. ALLES DAVOR IST VOM 30.6.
Die Variable, "p_aaer" und "new_p_aaer", "ch_cm" und "ch_cs" fallen durch extrem viele NaN-Werte auf und werden komplett entfernt. Die übrigen Variablen bestehen zu maximal ca 10% aus NaN-Werten und werden daher als unbedenklich eingestuft. "new_p_aaer" und alle Beobachtungen, welche NaN-Werte enthalten, werden beseitigt. DAZU NOCH EINE EXTRASPALTE "nan_frac" EINFÜGEN UND nan_col_list ZUM INDEX MACHEN. nan_number_list zu "NaN-Werte".

```{r}
data <- data[, -match(c("new_p_aaer", "p_aaer", "ch_cm", "ch_cs"), names(data))]
data <- data[complete.cases(data),]
dim(data) # Übrige Beobachtungen: 116478 auf 49 Variablen
```
Damit ein NN oder die LogReg Inputdaten verwerten kann, müssen diese zwischen 0 und 1 normiert sein. Und zwar Jahr für Jahr. 
```{r}
normalize <- function(col, na.rm = TRUE) {
    return((col- min(col)) /(max(col)-min(col)))
}
# as.data.frame(apply(df$name, normalize))
for (col in colnames(data))
{
  data[col] <- normalize(data[col])
}

```

trainings- und testdaten generieren (seed = immer noch 42)

```{r}
smp_size <- floor(0.70 * nrow(data))

train_ind <- sample(seq_len(nrow(data)), size = smp_size)

train <- data[train_ind, ]

true_frac <- sum(train[,7] == 1) / (smp_size - sum(train[,7] == 1)) #sagt aus, wie hoch Anteil der Betrugsfälle im Testdatenssatz ist


train_smote_object <- SMOTE(train[, -7], train[,7], K = 5, dup_size = 1 / true_frac)$data
train_smote_object$class <- as.numeric(train_smote_object$class)

test <- data[-train_ind, ]
```



```{r, include=TRUE}
#das sind nur nebenrechnungen

# Anteil der Betrugsfälle im Trainingsdatensatz vor SMOTE: 
sum(train[,7] == 1) / (smp_size - sum(train[,7] == 1))

# Anteil der Betrugsfälle im Trainingsdatensatz nach SMOTE: 
sum(train_smote_object$class == "1") / nrow(train_smote_object)

```

Jetzt werden 3 NN trainiert: Alle Daten, nur Roh, nur Ratios. seed = 42

-> DAS WIRD VON SEBBI GEMACHT


```{r}
# Funktion "evaluate basteln. Rechnet mit y_test und y_pred. 
# Ausgabe ist Tabelle mit Acc, Error, Prec, Sens, F1, F2. 
# Die Metriken werden nur für die True-Zeile (also fraud) berechnet.

# Confusion matrix sieht so aus: Spalten = pred(0,1), Zeilen = actual(0,1)

evaluate <- function(test, pred){
  
  pred <- ifelse(pred > 0.5, 1, 0)
  confusion <- table(test, pred)
  total_acc <- numeric(2)
  total_acc[1] <- NaN
  total_acc[2] <- round((confusion[1,1] + confusion[2,2]) / sum(confusion),4)

  
  prec <- numeric(2)
  prec[1] <- NaN
  prec[2] <- round(confusion[2,2] / (confusion[2,2] + confusion[2,1]),4)
  
  sens <- numeric(2)
  sens[1] <- NaN
  sens[2] <- round(confusion[2,2] / (confusion[2,2] + confusion[1,2]),4)
  
  F1 <- numeric(2)
  F1[1] <- NaN
  F1[2] <- round(2*(prec[2]*sens[2])/(prec[2] + sens[2]), 4)
  
  F2 <- numeric(2)
  F2[1] <- NaN
  F2[2] <- round((1 + 2^2) * (prec[2]*sens[2]) / (2^2 * prec[2] + sens[2]), 4)
  
  print(cbind(confusion, total_acc, prec, sens, F1, F2))
  
}


y_test <- c(1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1)
y_pred <- c(1,0,1,1,0,1,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1)

evaluate(test=y_test, pred=y_pred ) # sou, Evaluierungssfunktion steht. 
```

Logistische Regression basteln:
```{r}
# trainingsdaten in train_smote_object
# testdaten in test

mylogit <- glm(class ~., data = train_smote_object, family = "binomial")
y_pred_logit <- predict.glm(mylogit, test[, !names(train) %in% c('misstate')])

evaluate(test$misstate, y_pred_logit)
```
Scheinbar ist irgendetwas perfekt in den Traingsdaten korreliert. Herausfinden was

```{r}
round(cor(train_smote_object$class, train_smote_object[, !names(train_smote_object) %in% c('class')]), 2)
```
GEFUNDEN: ch_cm, ch_roa, dpi & ni haben eine Korrelation von 0. 
  -> spielt keine Rolle. 
  -> By the way: Hier wäre ein guter Ansatz zur Feature-Selection

TODO:
  -> PDF-Output muss für uns angepasst werden:
      -> HHU-Beschriftungen- und Bilder
      -> Unsere Namen
      -> Unsere Quellen & deren korrekte Zitation



Ich habe eben bei Bao egschaut. Unser Baseline modell hat eine prec von ca 45%. Bao war mit absolut allen Modellen DEUTLICH darunter (1-10%). Sind wir die Besten? Nein. Irgendwas ist hier falsch. Morgen nochmal schauen, wie sich LOGIT verhält, wenn sie nur Rohdaten bekommt (normalisiert und upsampled natürlich). Alternativ kann man sich zu dem Gedanken hinreißen lassen, dass Bao und co einfach nicht gut gearbeitet haben...mal schauen. 


AUFGABE: 
  -> Normalisierung der Daten innerhalb jeden Jahres
  -> [DONE] LogReg mit allen Daten 
  -> Log Reg mit nur Rohdaten
  -> Log Reg mit nur Ratios






















<!--chapter:end:04-methods.Rmd-->

`r if(knitr:::is_latex_output()) '\\startappendices'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 

<!-- If you feel it necessary to include an appendix, it goes here. The first appendix should include the commands above. -->


# The First Appendix

This first appendix includes an R chunk that was hidden in the document (using `echo = FALSE`) to help with readibility:

**In 02-rmd-basics-code.Rmd**

```{r ref.label='chunk-parts', eval=FALSE, echo = TRUE}
```

**And here's another one from the same chapter, i.e. Chapter \@ref(code):**

```{r ref.label='oxford-logo-rotated', eval=FALSE, echo = TRUE}
```



# The Second Appendix, for Fun

<!--chapter:end:front-and-back-matter/98-appendices.Rmd-->

`r if(!knitr:::is_latex_output()) '# References {-}'`

<!-- If you're outputting to LaTeX, the heading and references will be generated by the OxThesis LaTeX template. This .Rmd file serves only to add the References headline to gitbook output before  the references are added by pandoc -->

<!--chapter:end:front-and-back-matter/99-references.Rmd-->

