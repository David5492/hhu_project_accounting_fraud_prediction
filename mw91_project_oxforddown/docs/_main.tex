%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% OXFORD THESIS TEMPLATE

% Use this template to produce a standard thesis that meets the Oxford University requirements for DPhil submission
%
% Originally by Keith A. Gillow (gillow@maths.ox.ac.uk), 1997
% Modified by Sam Evans (sam@samuelevansresearch.org), 2007
% Modified by John McManigle (john@oxfordechoes.com), 2015
% Modified by Ulrik Lyngs (ulrik.lyngs@cs.ox.ac.uk), 2018-, for use with R Markdown
%
% Ulrik Lyngs, 25 Nov 2018: Following John McManigle, broad permissions are granted to use, modify, and distribute this software
% as specified in the MIT License included in this distribution's LICENSE file.
%
% John commented this file extensively, so read through to see how to use the various options.  Remember that in LaTeX,
% any line starting with a % is NOT executed.  Several places below, you have a choice of which line to use
% out of multiple options (eg draft vs final, for PDF vs for binding, etc.)  When you pick one, add a % to the beginning of
% the lines you don't want.


%%%%% PAGE LAYOUT
% The most common choices should be below.  You can also do other things, like replacing "a4paper" with "letterpaper", etc.

% This one formats for two-sided binding (ie left and right pages have mirror margins; blank pages inserted where needed):
%\documentclass[a4paper,twoside]{templates/ociamthesis}
% This one formats for one-sided binding (ie left margin > right margin; no extra blank pages):
%\documentclass[a4paper]{ociamthesis}
% This one formats for PDF output (ie equal margins, no extra blank pages):
%\documentclass[a4paper,nobind]{templates/ociamthesis}

% As you can see from the uncommented line below, oxforddown template uses the a4paper size, 
% and passes in the binding option from the YAML header in index.Rmd:
\documentclass[a4paper, nobind]{templates/ociamthesis}


%%%%% ADDING LATEX PACKAGES
% add hyperref package with options from YAML %
\usepackage[pdfpagelabels]{hyperref}
% change the default coloring of links to something sensible
\usepackage{xcolor}

\definecolor{myurlcolor}{RGB}{0,0,139}
\definecolor{mycitecolor}{RGB}{0,33,71}

\hypersetup{
  hidelinks,
  colorlinks,
  linkcolor=.,
  urlcolor=myurlcolor,
  citecolor=mycitecolor
}



% add float package to allow manual control of figure positioning %
\usepackage{float}

% enable strikethrough
\usepackage[normalem]{ulem}

% use soul package for correction highlighting
\usepackage{color, soul}
\definecolor{correctioncolor}{HTML}{CCCCFF}
\sethlcolor{correctioncolor}
\newcommand{\ctext}[3][RGB]{%
  \begingroup
  \definecolor{hlcolor}{#1}{#2}\sethlcolor{hlcolor}%
  \hl{#3}%
  \endgroup
}
\soulregister\ref7
\soulregister\cite7
\soulregister\autocite7
\soulregister\textcite7
\soulregister\pageref7

%%%%% FIXING / ADDING THINGS THAT'S SPECIAL TO R MARKDOWN'S USE OF LATEX TEMPLATES
% pandoc puts lists in 'tightlist' command when no space between bullet points in Rmd file,
% so we add this command to the template
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
 
% UL 1 Dec 2018, fix to include code in shaded environments
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

%UL set white space before and after code blocks
\renewenvironment{Shaded}
{
  \vspace{10pt}%
  \begin{snugshade}%
}{%
  \end{snugshade}%
  \vspace{8pt}%
}

% User-included things with header_includes or in_header will appear here
% kableExtra packages will appear here if you use library(kableExtra)
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}


%UL set section header spacing
\usepackage{titlesec}
% 
\titlespacing\subsubsection{0pt}{24pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}


%UL set whitespace around verbatim environments
\usepackage{etoolbox}
\makeatletter
\preto{\@verbatim}{\topsep=0pt \partopsep=0pt }
\makeatother



%%%%%%% PAGE HEADERS AND FOOTERS %%%%%%%%%
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\fancyhf{} % clear the header and footers
\pagestyle{fancy}
\renewcommand{\chaptermark}[1]{\markboth{\thechapter. #1}{\thechapter. #1}}
\renewcommand{\sectionmark}[1]{\markright{\thesection. #1}} 
\renewcommand{\headrulewidth}{0pt}

\fancyhead[LO]{\emph{\leftmark}} 
\fancyhead[RE]{\emph{\rightmark}} 

% UL page number position 
\fancyfoot[C]{\emph{\thepage}} %regular pages
\fancypagestyle{plain}{\fancyhf{}\fancyfoot[C]{\emph{\thepage}}} %chapter pages

% JEM fix header on cleared pages for openright
\def\cleardoublepage{\clearpage\if@twoside \ifodd\c@page\else
   \hbox{}
   \fancyfoot[C]{}
   \newpage
   \if@twocolumn\hbox{}\newpage
   \fi
   \fancyhead[LO]{\emph{\leftmark}} 
   \fancyhead[RE]{\emph{\rightmark}} 
   \fi\fi}


%%%%% SELECT YOUR DRAFT OPTIONS
% This adds a "DRAFT" footer to every normal page.  (The first page of each chapter is not a "normal" page.)

% IP feb 2021: option to include line numbers in PDF

% This highlights (in blue) corrections marked with (for words) \mccorrect{blah} or (for whole
% paragraphs) \begin{mccorrection} . . . \end{mccorrection}.  This can be useful for sending a PDF of
% your corrected thesis to your examiners for review.  Turn it off, and the blue disappears.
\correctionstrue


%%%%% BIBLIOGRAPHY SETUP
% Note that your bibliography will require some tweaking depending on your department, preferred format, etc.
% If you've not used LaTeX before, I recommend reading a little about biblatex/biber and getting started with it.
% If you're already a LaTeX pro and are used to natbib or something, modify as necessary.
% Either way, you'll have to choose and configure an appropriate bibliography format...


\usepackage[style=authoryear, sorting=nyt, backend=biber, maxcitenames=2, useprefix, doi=true, isbn=false, uniquename=false]{biblatex}
\newcommand*{\bibtitle}{Works Cited}

\addbibresource{references.bib}


% This makes the bibliography left-aligned (not 'justified') and slightly smaller font.
\renewcommand*{\bibfont}{\raggedright\small}


% Uncomment this if you want equation numbers per section (2.3.12), instead of per chapter (2.18):
%\numberwithin{equation}{subsection}


%%%%% THESIS / TITLE PAGE INFORMATION
% Everybody needs to complete the following:
\title{\texttt{oxforddown}:\\
An Oxford University Thesis\\
Template for R Markdown}
\author{Author Name}
\college{Your College}

% Master's candidates who require the alternate title page (with candidate number and word count)
% must also un-comment and complete the following three lines:

% Uncomment the following line if your degree also includes exams (eg most masters):
%\renewcommand{\submittedtext}{Submitted in partial completion of the}
% Your full degree name.  (But remember that DPhils aren't "in" anything.  They're just DPhils.)
\degree{Doctor of Philosophy}
% Term and year of submission, or date if your board requires (eg most masters)
\degreedate{Michaelmas 2018}


%%%%% YOUR OWN PERSONAL MACROS
% This is a good place to dump your own LaTeX macros as they come up.

% To make text superscripts shortcuts
	\renewcommand{\th}{\textsuperscript{th}} % ex: I won 4\th place
	\newcommand{\nd}{\textsuperscript{nd}}
	\renewcommand{\st}{\textsuperscript{st}}
	\newcommand{\rd}{\textsuperscript{rd}}

%%%%% THE ACTUAL DOCUMENT STARTS HERE
\begin{document}

%%%%% CHOOSE YOUR LINE SPACING HERE
% This is the official option.  Use it for your submission copy and library copy:
\setlength{\textbaselineskip}{22pt plus2pt}
% This is closer spacing (about 1.5-spaced) that you might prefer for your personal copies:
%\setlength{\textbaselineskip}{18pt plus2pt minus1pt}

% You can set the spacing here for the roman-numbered pages (acknowledgements, table of contents, etc.)
\setlength{\frontmatterbaselineskip}{17pt plus1pt minus1pt}

% UL: You can set the line and paragraph spacing here for the separate abstract page to be handed in to Examination schools
\setlength{\abstractseparatelineskip}{13pt plus1pt minus1pt}
\setlength{\abstractseparateparskip}{0pt plus 1pt}

% UL: You can set the general paragraph spacing here - I've set it to 2pt (was 0) so
% it's less claustrophobic
\setlength{\parskip}{2pt plus 1pt}

%
% Oxford University logo on title page
%
\def\crest{{\includegraphics[width=5cm]{templates/beltcrest.pdf}}}
\renewcommand{\university}{University of Oxford}
\renewcommand{\submittedtext}{A thesis submitted for the degree of}


% Leave this line alone; it gets things started for the real document.
\setlength{\baselineskip}{\textbaselineskip}


%%%%% CHOOSE YOUR SECTION NUMBERING DEPTH HERE
% You have two choices.  First, how far down are sections numbered?  (Below that, they're named but
% don't get numbers.)  Second, what level of section appears in the table of contents?  These don't have
% to match: you can have numbered sections that don't show up in the ToC, or unnumbered sections that
% do.  Throughout, 0 = chapter; 1 = section; 2 = subsection; 3 = subsubsection, 4 = paragraph...

% The level that gets a number:
\setcounter{secnumdepth}{2}
% The level that shows up in the ToC:
\setcounter{tocdepth}{1}


%%%%% ABSTRACT SEPARATE
% This is used to create the separate, one-page abstract that you are required to hand into the Exam
% Schools.  You can comment it out to generate a PDF for printing or whatnot.

% JEM: Pages are roman numbered from here, though page numbers are invisible until ToC.  This is in
% keeping with most typesetting conventions.
\begin{romanpages}

% Title page is created here
\maketitle

%%%%% DEDICATION -- If you'd like one, un-comment the following.
\begin{dedication}
  For Yihui Xie
\end{dedication}

%%%%% ACKNOWLEDGEMENTS -- Nothing to do here except comment out if you don't want it.
\begin{acknowledgements}
 	This is where you will normally thank your advisor, colleagues, family and friends, as well as funding and institutional support. In our case, we will give our praises to the people who developed the ideas and tools that allow us to push open science a little step forward by writing plain-text, transparent, and reproducible theses in R Markdown.

  We must be grateful to John Gruber for inventing the original version of Markdown, to John MacFarlane for creating Pandoc (\url{http://pandoc.org}) which converts Markdown to a large number of output formats, and to Yihui Xie for creating \texttt{knitr} which introduced R Markdown as a way of embedding code in Markdown documents, and \texttt{bookdown} which added tools for technical and longer-form writing.

  Special thanks to \href{http://chester.rbind.io}{Chester Ismay}, who created the \texttt{thesisdown} package that helped many a PhD student write their theses in R Markdown. And a very special thanks to John McManigle, whose adaption of Sam Evans' adaptation of Keith Gillow's original maths template for writing an Oxford University DPhil thesis in LaTeX provided the template that I in turn adapted for R Markdown.

  Finally, profuse thanks to JJ Allaire, the founder and CEO of \href{http://rstudio.com}{RStudio}, and Hadley Wickham, the mastermind of the tidyverse without whom we'd all just given up and done data science in Python instead. Thanks for making data science easier, more accessible, and more fun for us all.

  \begin{flushright}
  Ulrik Lyngs \\
  Linacre College, Oxford \\
  2 December 2018
  \end{flushright}
\end{acknowledgements}


%%%%% ABSTRACT -- Nothing to do here except comment out if you don't want it.
\begin{abstract}
	This \emph{R Markdown} template is for writing an Oxford University thesis. The template is built using Yihui Xie's \texttt{bookdown} package, with heavy inspiration from Chester Ismay's \texttt{thesisdown} and the \texttt{OxThesis} \LaTeX~template (most recently adapted by John McManigle).

 This template's sample content include illustrations of how to write a thesis in R Markdown, and largely follows the structure from \href{https://ulyngs.github.io/rmarkdown-workshop-2019/}{this R Markdown workshop}.

 Congratulations for taking a step further into the lands of open, reproducible science by writing your thesis using a tool that allows you to transparently include tables and dynamically generated plots directly from the underlying data. Hip hooray!
\end{abstract}

%%%%% MINI TABLES
% This lays the groundwork for per-chapter, mini tables of contents.  Comment the following line
% (and remove \minitoc from the chapter files) if you don't want this.  Un-comment either of the
% next two lines if you want a per-chapter list of figures or tables.
  \dominitoc % include a mini table of contents

% This aligns the bottom of the text of each page.  It generally makes things look better.
\flushbottom

% This is where the whole-document ToC appears:
\tableofcontents

\listoffigures
	\mtcaddchapter
  	% \mtcaddchapter is needed when adding a non-chapter (but chapter-like) entity to avoid confusing minitoc

% Uncomment to generate a list of tables:
\listoftables
  \mtcaddchapter
%%%%% LIST OF ABBREVIATIONS
% This example includes a list of abbreviations.  Look at text/abbreviations.tex to see how that file is
% formatted.  The template can handle any kind of list though, so this might be a good place for a
% glossary, etc.
% First parameter can be changed eg to "Glossary" or something.
% Second parameter is the max length of bold terms.
\begin{mclistof}{List of Abbreviations}{3.2cm}

\item[1-D, 2-D]

One- or two-dimensional, referring \textbf{in this thesis} to spatial dimensions in an image.

\item[Otter]

One of the finest of water mammals.

\item[Hedgehog]

Quite a nice prickly friend.

\end{mclistof} 


% The Roman pages, like the Roman Empire, must come to its inevitable close.
\end{romanpages}

%%%%% CHAPTERS
% Add or remove any chapters you'd like here, by file name (excluding '.tex'):
\flushbottom

% all your chapters and appendices will appear here
\hypertarget{intro}{%
\chapter{Einleitung}\label{intro}}

Vergangene und gegenwärtige Bilanzskandale wie ENRON, Worldcom oder Wirecard führen immer wieder zu Diskussionen über die Vertrauenswürdigkeit des Kapitalmarkts sowie über die Vertrauenswürdigkeit der Abschlussprüfung. Bilanzskandale führen weit-reichende Konsequenten mit sich welche eine genauere Analyse des Warums und wie anstoßen (vgl. Boecker/Zwirner 2012, S. 1).
Diese Arbeit hat das Ziel, den Einsatz neuronaler Netze gegenüber der gängigeren Me-thode der logistischen Regression (vgl. Bao et al., 2020) zu vergleichen und dabei her-auszufinden, ob erstere signifikant besser darin sind, so viele Betrugsfälle wie möglich zu identifizieren, ohne die triviale Annahme zu treffen, dass jeder Fall ein Betrugsfall ist. Bei der Modellevaluation wird angenommen, dass ein nicht-identifizierter Betrugsfall doppelt so schwer wiegt, wie ein prognostizierter Betrugsfall, der sich als Nicht-Betrug herausstellt.
Die Metrik zur Messung der Ergebnisse ist der \(F_{\beta}\)-Score (vgl. Tharwat 2020, S. 174). Dieser bildet das harmonische Mittel aus Präzision und Sensitivität. Die Präzision sagt aus, wie viele vorhergesagte Betrugsfälle tatsächlich Betrugsfälle sind, wobei die Sensi-tivität dabei auf die Frage antwortet, wie viele der tatsächlichen Betrugsfälle als solche identifiziert wurden. Weil die Sensitivität im Kontext der Fraud Detection bedeutungs-voller erscheint, wird sie in dieser Arbeit doppelt so hoch gewichtet wie die Präzision. Die Formel dieser Metrik lautet dabei:
\[F_{\beta} = (1+\beta^{2})*\frac{P*R}{\beta^{2} * P+R}\]

\hypertarget{AccFraud_and_ML}{%
\chapter{Accounting Fraud und Machine Learning}\label{AccFraud_and_ML}}

Das Handelsgesetzbuch (HGB) sieht gemäß § 317 Absatz (Abs.) 1 Satz (S.) 3 vor, dass der Abschlussprüfer im Rahmen der Abschlussprüfung, Unrichtigkeiten und Verstöße welche der Ordnungsmäßigkeit des Abschlusses entgegen stehen, erkennt und entsprechend deklariert. Was unter den Begrifflichkeiten Unrichtigkeit und Verstöße zu verstehen ist, wird durch den Gesetzgeber an dieser Stelle nicht weiter konkretisiert (vgl. Zwernemann et al.~2015, S. 22; § 317 Abs.1 HGB).

Dem entgegen versucht das Institut der Wirtschaftsprüfer (IDW) mit dem veröffentlichten Prüfungsstandard 210 (IDW PS 210) Rechnung zu tragen. Diesem zu entnehmen ist, dass ein Fehlerhafter Abschluss entweder auf Fraud (Verstoß) oder Error (Unrichtigkeit) zurückzuführen ist. Unter dem Begriff Unrichtigkeit wird eine unabsichtliche Angabe im Abschluss verstanden. Konkret bedeutet dies begangene Rechenfehler, eine unbewusst falsche Anwendung von Rechnungslegungsgrundsätzen sowie die falsche Einschätzung von Sachverhalten (vgl. Hlavica et al.~2016, S. 209f.). Der Begriff Verstoß dagegen umfasst eine beabsichtigte Handlung mit dem Ziel rechtswidrige Vorteile zu realisieren. Diese Handlungen konkretisiert der IDW PS 210 als Vermögensschädigungen, Täuschungen und Gesetzesverstöße, welche eine Auswirkung auf die Rechnungslegung zur Folge haben (vgl. Zwernemann et al.~2015, S. 8).
Um die Gründe für eine betrügerische Handlung nachvollziehen zu können, entwickelte Donald Cressey in den 1940er Jahren das sogenannte „Fraud-Triangle``. Dieses Dreieck wird ferner dem IDW PS 210 zugrunde gelegt (vgl. Boecker/Zwirner 2012, S. 2f.). Demnach tritt ein Verstoße dann auf, wenn drei Gegebenheiten als erfüllt angesehen werden können. So muss der Täter eine Gelegenheit zu der Tat haben und einen Anreiz (Motivation) für die Tat verspüren. Als letztes muss der Täter die Tat als moralisch akzeptabel rechtfertigen vor sich selbst rechtfertigen (vgl. Schuchter/Levi 2016, S. 3f.).
Aber nicht nur durch psychologische Ansätze versucht die Wissenschaft Verstöße einzuordnen und zu identifizieren, sondern auch durch eine Vielzahl an Machine Learning Ansätzen, welche Verstöße mittels Algorithmen identifizieren sollen.
Vor dem Hintergrund der potenziellen Gefahren des Bilanzbetrugs werden Letztere zunehmend für die Vorhersage und Aufdeckung von diskretionärer Bilanzpolitik eingesetzt. Die Benchmark in diesem Bereich ist das Dechow et al.~Modell, welches auf Grundlage von Accounting and Auditing Enforcement Releases (AAERs) der U.S. Securities and Exchange Comission (SEC) mit Hilfe einer logistischen Regression die Wahrscheinlichkeiten von (bewusst) fehlerhaften Darstellungen schätzt und klassifiziert (Dechow et al.~2011). Hierbei gelten die AAERs als ProxyVariable für die Manipulation der Bilanz. Durch das Voraussetzen der Untersuchungshandlungen seitens der SEC ergibt sich der Vorteil, dass der Typ I Fehler -- das Modell sagt fälschlicherweise ein misstatement voraus -- deutlich geringer ausfällt. Durch einige wenige Transformationen der logistischen Funktion kann der Einfluss einer jeden unabhängigen Variable durch den entsprechenden Regressionskoeffizienten hinsichtlich der Effektgröße verglichen werden, weswegen die Ergebnisse gut interpretierbar sind. Aus dem Dechow et al.~Modell folgt eine korrekte Klassifizierung von misstatements und nonmisstatements von ungefähr 63\% (Dechow et al.~2011, S.59). Die Sensitivität, d.h. in wie vielen Fällen das Modell einen misstatement richtig vorhergesagt hat, liegt bei etwas mehr als 68\% (339 von 494). Der Typ II Fehler (das Modell klassifiziert ein misstatement als nonmisstatement), der im Rahmen des accounting frauds schwerwiegender ist als der Typ I Fehler (vgl. Lin et al.~2015, S. 468), liegt bei etwas mehr als 31\% (155 von 494).
Ein Vergleich der Performance der logistischen Regression mit den neuronalen Netzen, einer weiteren Methode zur Vorhersage von Bilanzbetrug, findet sich in dem Paper von Lin et al.~(2015). Aus diesem geht hervor, dass die neuronalen Netze hinsichtlich der Aufdeckung von accounting fraud bessere Ergebnisse liefern als die logistische Regression. Die artificial neural networks (ANNs) erreichen bei dem Testdatensatz eine Genauigkeit von fast 93\%. Die Sensitivität von fast 83\% liegt zudem deutlich höher als bei der logistischen Regression, bei der 72\% der misstatements richtig vorhergesagt wurden (vgl. Lin et al.~2015, S. 465f.).
Die Interpretierbarkeit und verhältnismäßig einfache Anwendbarkeit haben die logistische Regression zu einem beliebten Instrument gemacht, die Ergebnisse hinsichtlich der Vorhersage von Bilanzbetrug werden allerdings von anderen Modellen übertroffen (vgl. Dutta et al.~2017, S. 375). Im Falle der neuronalen Netze ergibt sich wiederum der Nachteil der geringeren Transparenz hinsichtlich der Arbeitsweise des Algorithmus (vgl. Bao et al.~2020, S. 228). Schlussendlich ergibt sich ein Trade-Off zwischen der Interpretierbarkeit und Vorhersagekraft. Nachfolgend liegt der Fokus dieser Ausarbeitung auf der reinen Performance respektive Vorhersagekraft des Modells. Ein Modell, welches misstatements richtig vorhersagt, erscheint bei der Klassifizierung von Bilanzbetrug wichtiger als ein Modell, aus dem abgelesen werden kann, welche Variablen den misstatement wie stark beeinflussen. An dieser Stelle sollte erwähnt werden, dass auch ein Modell, welches in 99\% der Fälle die richtige Vorhersage trifft, hinsichtlich der Aufdeckung von accounting fraud nicht geeignet sein muss. Durch die Problematik der signifikanten sample imbalance -- Betrugsfälle sind stark unterrepräsentiert -- ist es möglich, dass misstatements durch das Modell nicht erkannt respektive falsch klassifiziert werden, also Typ II Fehler auftreten können.

\hypertarget{dataset}{%
\chapter{Datensatz}\label{dataset}}

Ein besonderer Fokus liegt aus diesem Grund auf der Minimierung des Typ II Fehlers respektive der Maximierung der Sensitivität des Modells. Eine höchstmögliche Genauigkeit ist gut, aufgrund der sehr geringen Anzahl an bilanziellen Verfehlungen verglichen zu der Stichprobengröße sollte hier allerdings kein Schwerpunkt liegen.
Zum Training und Test des Machine Learning Modells, wird ein Datensatz aus der Veröffentlichung von Bao et al.~(2020) verwendet. Die Autoren haben diesen im Internet auf der Seite „GitHub`` zur Verfügung gestellt (vgl. Bao et al.~2020, GitHub Repository). Dieser besteht aus allen öffentlich gelisteten US-amerikanischen Firmen im Zeitraum von 1991 bis 2008. Die Accounting-Betrugsfälle aus den „Accounting and Auditing Enforcement Releases`` (AAER), die von der United States Securities and Exchange Commission (SEC) im gleichen Zeitraum veröffentlicht worden sind (vgl. Bao et al.~2020, S. 207). Der Datensatz listet für jeden Eintrag 28 verschiedene finanzielle Items auf. Diese setzen sich aus den Veröffentlichungen von Cecchini et al.~(2010) und Dechow et al.~(2011) zusammen. Die finanziellen Items stammen aus vier verschiedenen Bereichen, der Bilanz (z.B. gesamte Forderungen), der Gewinn- und Verlustrechnung (z.B. Nettoumsatz), der Kapitalflussrechnung (z.B. Langzeitemission von Schuldtiteln) und dem Marktwert (z.B. Common Shares Outstanding).
Da Accounting-Betrugsfälle eher seltener vorkommen (vgl. Dutta et al.~2011, S. 381), weist der Datensatz eine große Verteilungsungleichheit zwischen den Betrugs- und Nicht-betrugsfällen auf. Weniger als ein Prozent aller Einträge im Datensatz sind hierbei Betrugsfälle. Weitere Betrugsfälle könnten über andere Datenbanken gesucht werden, hierbei gibt es allerdings das Problem, dass die Betrugsfälle identifiziert und einzeln herausgesucht werden müssen. Daher besteht hier ein „class imbalance`` Problem, das mithilfe des Machine Learning Algorithmus gelöst werden muss.
Zudem sind nur Betrugsfälle bis zum Jahr 2008 in diesem Datensatz erhalten, da die SEC nach der Finanzkrise ihre Prioritäten geändert hat (vgl. Bao et al.~2020, S. 208). Sollte sich die Art und Weise mit der Accounting-Betrug durchgeführt wird in den Jahren danach geändert haben, so können diese Fälle unter Umständen nicht vom Algorithmus erkannt werden.

AB HIER HANDELT SICH DER AUFSCHRIEB MEHR UM NOTIZEN ALS UM EINE ABGABEFÄHIGE VERSION.

Cleaning-Prozess-Reichenfolge:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Daten als ``data'' Laden
\item
  Spalten ``p\_aaer'' und ``new\_p\_aaer'' löschen
\item
  Alle Zeilen mit NaN-Werten löschen
\item
  all\_data bilden: Besteht nur aus 14 + 28 + 2 Vars
\item
  all\_data via Jahreszahl normalisieren. fyear droppen.
\item
  raw\_data (28 + 1 Vars) und ratio\_data (14 + 1 Vars) aus all\_data bilden
\item
  deskriptive Statistiken können mit den Datensätzen: min, Max, Mean, Median 0.25 und 0.75 Quantile UND normalisierte Boxplots und Histogramme. Ggf noch Ausreißer raus.
\end{enumerate}

Dann ist die Datenvorbereitung fertig und man kann Modelle damit rechnen. Bis zu dem Punkt geht die nächste Zelle:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# 1.}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{import}\NormalTok{(}\StringTok{"data/uscecchini28.csv"}\NormalTok{)}

\DocumentationTok{\#\# 2.}
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ data[,}\SpecialCharTok{{-}}\FunctionTok{match}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"p\_aaer"}\NormalTok{, }\StringTok{"new\_p\_aaer"}\NormalTok{), }\FunctionTok{names}\NormalTok{(data))]}

\DocumentationTok{\#\# 3.}
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ data[}\FunctionTok{complete.cases}\NormalTok{(data),]}

\DocumentationTok{\#\# 4.}
\NormalTok{all\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"fyear"}\NormalTok{, }\StringTok{"misstate"}\NormalTok{, }\StringTok{"act"}\NormalTok{, }\StringTok{"ap"}\NormalTok{, }\StringTok{"at"}\NormalTok{, }\StringTok{"ceq"}\NormalTok{, }\StringTok{"che"}\NormalTok{, }
               \StringTok{"cogs"}\NormalTok{, }\StringTok{"csho"}\NormalTok{, }\StringTok{"dlc"}\NormalTok{, }\StringTok{"dltis"}\NormalTok{, }\StringTok{"dltt"}\NormalTok{, }\StringTok{"dp"}\NormalTok{, }\StringTok{"ib"}\NormalTok{, }
               \StringTok{"invt"}\NormalTok{, }\StringTok{"ivao"}\NormalTok{, }\StringTok{"ivst"}\NormalTok{, }\StringTok{"lct"}\NormalTok{, }\StringTok{"lt"}\NormalTok{, }\StringTok{"ni"}\NormalTok{, }\StringTok{"ppegt"}\NormalTok{, }
               \StringTok{"pstk"}\NormalTok{, }\StringTok{"re"}\NormalTok{, }\StringTok{"rect"}\NormalTok{, }\StringTok{"sale"}\NormalTok{, }\StringTok{"sstk"}\NormalTok{, }\StringTok{"txp"}\NormalTok{, }\StringTok{"txt"}\NormalTok{, }
               \StringTok{"xint"}\NormalTok{, }\StringTok{"prcc\_f"}\NormalTok{, }\StringTok{"dch\_wc"}\NormalTok{, }\StringTok{"ch\_rsst"}\NormalTok{, }\StringTok{"dch\_rec"}\NormalTok{, }
               \StringTok{"dch\_inv"}\NormalTok{, }\StringTok{"soft\_assets"}\NormalTok{, }\StringTok{"dpi"}\NormalTok{, }\StringTok{"ch\_cs"}\NormalTok{, }\StringTok{"ch\_cm"}\NormalTok{, }
               \StringTok{"ch\_roa"}\NormalTok{, }\StringTok{"ch\_fcf"}\NormalTok{, }\StringTok{"reoa"}\NormalTok{, }\StringTok{"EBIT"}\NormalTok{, }\StringTok{"issue"}\NormalTok{, }\StringTok{"bm"}\NormalTok{)}
\NormalTok{all\_data }\OtherTok{\textless{}{-}}\NormalTok{ data[, }\FunctionTok{match}\NormalTok{(all\_names, }\FunctionTok{names}\NormalTok{(data))]}

\DocumentationTok{\#\# 5. }
\CommentTok{\# Funktion schreiben}
\NormalTok{normalize }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x)\{}
  \FunctionTok{return}\NormalTok{((x }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(x)) }\SpecialCharTok{/}\NormalTok{ (}\FunctionTok{max}\NormalTok{(x) }\SpecialCharTok{{-}} \FunctionTok{min}\NormalTok{(x)))}
\NormalTok{\}}
\CommentTok{\# Funktion anwenden}
\ControlFlowTok{for}\NormalTok{ (year }\ControlFlowTok{in} \FunctionTok{unique}\NormalTok{(all\_data}\SpecialCharTok{$}\NormalTok{fyear))\{}
  \ControlFlowTok{for}\NormalTok{ (col }\ControlFlowTok{in} \FunctionTok{names}\NormalTok{(all\_data))\{}
\NormalTok{    all\_data[data}\SpecialCharTok{$}\NormalTok{fyear }\SpecialCharTok{==}\NormalTok{ year, col] }\OtherTok{\textless{}{-}} \FunctionTok{normalize}\NormalTok{(}
\NormalTok{      all\_data[data}\SpecialCharTok{$}\NormalTok{fyear }\SpecialCharTok{==}\NormalTok{ year, col]}
\NormalTok{      ) }
\NormalTok{  \}}
\NormalTok{\}}
\CommentTok{\# drop fyear{-}Variable}
\NormalTok{all\_data }\OtherTok{\textless{}{-}}\NormalTok{ all\_data[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{] }

\DocumentationTok{\#\# 6. }
\NormalTok{raw\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"misstate"}\NormalTok{, }\StringTok{"act"}\NormalTok{, }\StringTok{"ap"}\NormalTok{, }\StringTok{"at"}\NormalTok{, }\StringTok{"ceq"}\NormalTok{, }\StringTok{"che"}\NormalTok{, }\StringTok{"cogs"}\NormalTok{, }
               \StringTok{"csho"}\NormalTok{, }\StringTok{"dlc"}\NormalTok{, }\StringTok{"dltis"}\NormalTok{, }\StringTok{"dltt"}\NormalTok{, }\StringTok{"dp"}\NormalTok{, }\StringTok{"ib"}\NormalTok{, }\StringTok{"invt"}\NormalTok{, }
               \StringTok{"ivao"}\NormalTok{, }\StringTok{"ivst"}\NormalTok{, }\StringTok{"lct"}\NormalTok{, }\StringTok{"lt"}\NormalTok{, }\StringTok{"ni"}\NormalTok{, }\StringTok{"ppegt"}\NormalTok{, }\StringTok{"pstk"}\NormalTok{, }
               \StringTok{"re"}\NormalTok{, }\StringTok{"rect"}\NormalTok{, }\StringTok{"sale"}\NormalTok{, }\StringTok{"sstk"}\NormalTok{, }\StringTok{"txp"}\NormalTok{, }\StringTok{"txt"}\NormalTok{, }\StringTok{"xint"}\NormalTok{, }
               \StringTok{"prcc\_f"}\NormalTok{)}

\NormalTok{ratio\_names }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"misstate"}\NormalTok{, }\StringTok{"dch\_wc"}\NormalTok{, }\StringTok{"ch\_rsst"}\NormalTok{, }\StringTok{"dch\_rec"}\NormalTok{, }
                 \StringTok{"dch\_inv"}\NormalTok{, }\StringTok{"soft\_assets"}\NormalTok{, }\StringTok{"dpi"}\NormalTok{, }\StringTok{"ch\_cs"}\NormalTok{, }\StringTok{"ch\_cm"}\NormalTok{, }
                 \StringTok{"ch\_roa"}\NormalTok{, }\StringTok{"ch\_fcf"}\NormalTok{, }\StringTok{"reoa"}\NormalTok{, }\StringTok{"EBIT"}\NormalTok{, }\StringTok{"issue"}\NormalTok{, }\StringTok{"bm"}\NormalTok{)}

\NormalTok{raw\_data }\OtherTok{\textless{}{-}}\NormalTok{ all\_data[, }\FunctionTok{match}\NormalTok{(raw\_names, }\FunctionTok{names}\NormalTok{(all\_data))]}
\NormalTok{ratio\_data }\OtherTok{\textless{}{-}}\NormalTok{ all\_data[, }\FunctionTok{match}\NormalTok{(ratio\_names, }\FunctionTok{names}\NormalTok{(all\_data))]}

\CommentTok{\# ==================================================================| Maximale Breite bis zum Strich}

\DocumentationTok{\#\# 7. Statistiken erstellen. SKIP. }
\end{Highlighting}
\end{Shaded}

\hypertarget{methods}{%
\chapter{Methoden}\label{methods}}

Diese Arbeit vergleicht die Vorhersagequalität der in der Accounting Fraud Detection gängigen logistischen Regression nach Dechow et al.~(vgl. Bao et al.~2020, S. 2) mit der von neuronalen Netzen.
Die logistische Regression ist eng mit der linearen Regression verwandt und wird zur binären Schätzung einer Klassenzugehörigkeit verwandt (vgl. Géron 2019, S. 144). Dabei berechnet sie eine gewichtete Summe von Inputfaktoren und aggregiert sie zu einer Wahrscheinlichkeit zwischen 0 und 1. Liegt diese Wahrscheinlichkeit bei mindestens 0.5, so wird die Klasse „1`` vorhergesagt, welcher in dieser Arbeit der Klasse „fraud`` entspricht (vgl. Géron 2019, S. 144). Hierzu wird für jedes Attribut einer Beobachtung eine Sigmoid-Funktion verwendet, welche S-förmig vom Minimum bis zum Maximum des jeweiligen Attributs verläuft und die Verteilungen der Merkmalsausprägungen möglichst gut nach Klassenzugehörigkeit abgrenzt. Je weiter eine Merkmalsausprägung von dieser Grenze entfernt ist, desto näher ist der Funktionswert an der 1 oder der 0 (vgl. Géron 2019, S. 148).
Neuronale Netze bestehen aus drei Sorten von Schichten: Input-Schichten, welche Daten einlesen, versteckte Schichten, welche die Daten verarbeiten und Output-Schichten, welche aus den verarbeiteten Daten eine Prognose ableiten (vgl. Géron 2019, S. 286). In dieser Arbeit besteht die Output-Schicht aus lediglich einem Knoten, welche die Klassen „fraud`` abbildet. Die Anzahl der Knoten der Input-Schicht entspricht der Anzahl der Variablen im Datensatz. Die Knoten einer Schicht sind jeweils mit jedem Knoten seiner nachfolgenden Schicht durch „Gewichte`` verbunden. Jeder einzelne Knoten aggregiert die Signale, die er empfängt über deren Gewichte zu einer Zahl und wendet eine Aktivierungsfunktion an (vgl. Géron 2019, S. 282). Übersteigt der Funktionswert einen gegeben Schwellenwert, so „feuert`` das Neuron, was bedeutet, dass es ein Signal größer 0 an die Neuronen der nächsten Schicht weitergibt (vgl. Géron 2019, S. 282-283). Die Gewichte und alle Schwellenwerte werden durch Backpropagation unter Zuhilfenahme des Gradient Descent Algorithmus verbessert (vgl. Géron 2019, S.119 und S, 286). Sind alle Trainingsdaten einmal zum Training herangezogen worden, bedeutet das, dass das Netz für „eine Epoche`` trainiert wurde (vgl. Géron 2019, S. 127). In dieser Arbeit wird ein Netz über 100 Epochen hinweg trainiert.

AB HIER HANDELT SICH DER AUFSCHRIEB MEHR UM NOTIZEN ALS UM EINE ABGABEFÄHIGE VERSION.

TODO:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Evaluate-Funktion bilden
\item
  LogReg mit allen Daten. Ablauf:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    Train-test-Split (X\_train, y\_train, X\_test, y\_test)
  \item
    SOMTE in X\_train anwenden, seed = 42
  \item
    Training
  \item
    Prediction (in y\_pred speichern)
  \item
    Evaluate-Funktion anwenden
  \end{enumerate}
\item
  LogReg mit Rohdaten

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \ldots{}
  \end{enumerate}
\item
  LogReg mit Ratios

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    \ldots{}
  \end{enumerate}
\item
  Summary von allen 3 Modellen
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# 1. EVALUATE BILDEN}
\NormalTok{evaluate }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(test, pred, }\AttributeTok{border =} \FloatTok{0.5}\NormalTok{)\{}
  
\NormalTok{  pred }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(pred }\SpecialCharTok{\textgreater{}}\NormalTok{ border, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{  confusion }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(test, pred)}
\NormalTok{  TN }\OtherTok{\textless{}{-}}\NormalTok{ confusion[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{  TP }\OtherTok{\textless{}{-}}\NormalTok{ confusion[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{  FP }\OtherTok{\textless{}{-}}\NormalTok{ confusion[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{  FN }\OtherTok{\textless{}{-}}\NormalTok{ confusion[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]}

\NormalTok{  total\_acc }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  total\_acc[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{  total\_acc[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{((TN }\SpecialCharTok{+}\NormalTok{ TP) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(confusion),}\DecValTok{4}\NormalTok{)}

\NormalTok{  prec }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  prec[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{  prec[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(TP }\SpecialCharTok{/}\NormalTok{ (TP }\SpecialCharTok{+}\NormalTok{ FP),}\DecValTok{4}\NormalTok{)}
  
\NormalTok{  sens }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  sens[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{  sens[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(TP }\SpecialCharTok{/}\NormalTok{ (TP }\SpecialCharTok{+}\NormalTok{ FN),}\DecValTok{4}\NormalTok{)}
  
\NormalTok{  F1 }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  F1[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{  F1[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\DecValTok{2}\SpecialCharTok{*}\NormalTok{(prec[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{*}\NormalTok{sens[}\DecValTok{2}\NormalTok{])}\SpecialCharTok{/}\NormalTok{(prec[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ sens[}\DecValTok{2}\NormalTok{]), }\DecValTok{4}\NormalTok{)}

\NormalTok{  F.score }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(beta, }\AttributeTok{p =}\NormalTok{ prec[}\DecValTok{2}\NormalTok{], }\AttributeTok{s =}\NormalTok{ sens[}\DecValTok{2}\NormalTok{])\{}
    \FunctionTok{round}\NormalTok{((}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ beta}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\NormalTok{(p}\SpecialCharTok{*}\NormalTok{s)}\SpecialCharTok{/}\NormalTok{(beta}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{*}\NormalTok{p }\SpecialCharTok{+}\NormalTok{ s),}\DecValTok{4}\NormalTok{)}
\NormalTok{  \}}
  
\NormalTok{  F2 }\OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  F2[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{  F2[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{F.score}\NormalTok{(}\DecValTok{2}\NormalTok{)}
  
\NormalTok{  F}\FloatTok{.5} \OtherTok{\textless{}{-}} \FunctionTok{numeric}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{  F}\FloatTok{.5}\NormalTok{[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \ConstantTok{NaN}
\NormalTok{  F}\FloatTok{.5}\NormalTok{[}\DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{F.score}\NormalTok{(}\FloatTok{0.5}\NormalTok{)}
  
  \FunctionTok{return}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(confusion, total\_acc, prec, sens, F1, F2, F}\FloatTok{.5}\NormalTok{))}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# 2. MODELL MIT ALLEN DATEN TRANIEREN}
\NormalTok{train\_test\_split\_smote}\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(data, y\_col, }\AttributeTok{frac =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{seed =} \DecValTok{42}\NormalTok{, }
                                  \AttributeTok{k =} \DecValTok{5}\NormalTok{)\{}
  \FunctionTok{set.seed}\NormalTok{(seed)}
  
\NormalTok{  smp\_size }\OtherTok{\textless{}{-}} \FunctionTok{floor}\NormalTok{(frac }\SpecialCharTok{*} \FunctionTok{nrow}\NormalTok{(data))}
\NormalTok{  train\_ind }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{seq\_len}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(data)), }\AttributeTok{size =}\NormalTok{ smp\_size)}
  
\NormalTok{  X\_smote }\OtherTok{\textless{}{-}}\NormalTok{ data[train\_ind, }\SpecialCharTok{{-}}\FunctionTok{match}\NormalTok{(y\_col, }\FunctionTok{names}\NormalTok{(data))]}
\NormalTok{  y\_smote }\OtherTok{\textless{}{-}}\NormalTok{ data[train\_ind, }\FunctionTok{match}\NormalTok{(y\_col, }\FunctionTok{names}\NormalTok{(data))]}
  
\NormalTok{  true\_frac }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(y\_smote) }\SpecialCharTok{/} \FunctionTok{length}\NormalTok{(y\_smote)}
  
\NormalTok{  train\_smote\_object }\OtherTok{\textless{}{-}} \FunctionTok{SMOTE}\NormalTok{(X\_smote, y\_smote, }\AttributeTok{K =} \DecValTok{5}\NormalTok{, }
                              \AttributeTok{dup\_size =} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ true\_frac)}\SpecialCharTok{$}\NormalTok{data}
  
\NormalTok{  X\_train }\OtherTok{\textless{}{-}}\NormalTok{ train\_smote\_object[,}\SpecialCharTok{{-}}\FunctionTok{match}\NormalTok{(}\StringTok{\textquotesingle{}class\textquotesingle{}}\NormalTok{, }
                                        \FunctionTok{names}\NormalTok{(train\_smote\_object))]}
\NormalTok{  y\_train }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(train\_smote\_object[,}
                        \FunctionTok{match}\NormalTok{(}\StringTok{\textquotesingle{}class\textquotesingle{}}\NormalTok{, }\FunctionTok{names}\NormalTok{(train\_smote\_object))])}
  
\NormalTok{  X\_test }\OtherTok{\textless{}{-}}\NormalTok{ data[}\SpecialCharTok{{-}}\NormalTok{train\_ind, }\SpecialCharTok{{-}}\FunctionTok{match}\NormalTok{(y\_col, }\FunctionTok{names}\NormalTok{(data))]}
\NormalTok{  y\_test }\OtherTok{\textless{}{-}}\NormalTok{ data[}\SpecialCharTok{{-}}\NormalTok{train\_ind, }\FunctionTok{match}\NormalTok{(y\_col, }\FunctionTok{names}\NormalTok{(data))]}

  \FunctionTok{return}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{X\_train =}\NormalTok{ X\_train, }\AttributeTok{X\_test =}\NormalTok{ X\_test, }\AttributeTok{y\_train =}\NormalTok{ y\_train, }
              \AttributeTok{y\_test =}\NormalTok{ y\_test))}
\NormalTok{\}}

\NormalTok{splitted\_data }\OtherTok{\textless{}{-}} \FunctionTok{train\_test\_split\_smote}\NormalTok{(}\AttributeTok{data =}\NormalTok{ all\_data, }
                                    \AttributeTok{y\_col =} \StringTok{\textquotesingle{}misstate\textquotesingle{}}\NormalTok{, }\AttributeTok{frac =} \FloatTok{0.7}\NormalTok{)}

\NormalTok{X\_train }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{X\_train}
\NormalTok{X\_test }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{X\_test}
\NormalTok{y\_train }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{y\_train}
\NormalTok{y\_test }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{y\_test}

\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ X\_train}
\NormalTok{train}\SpecialCharTok{$}\NormalTok{misstate }\OtherTok{\textless{}{-}}\NormalTok{ y\_train}

\NormalTok{logit\_all\_data }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(misstate }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train, }
                      \AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\NormalTok{y\_pred\_logit\_all\_data }\OtherTok{\textless{}{-}} \FunctionTok{predict.glm}\NormalTok{(logit\_all\_data, }
\NormalTok{                                     X\_test, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{ )}

\FunctionTok{evaluate}\NormalTok{(y\_test, y\_pred\_logit\_all\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       0     1 total_acc   prec   sens     F1     F2    F.5
## 0 24068 10624       NaN    NaN    NaN    NaN    NaN    NaN
## 1    91   161    0.6934 0.0149 0.6389 0.0291 0.0681 0.0185
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tpr }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}
\NormalTok{pre }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.001}\NormalTok{, }\FloatTok{0.999}\NormalTok{, }\FloatTok{0.01}\NormalTok{))\{}
\NormalTok{  res }\OtherTok{\textless{}{-}} \FunctionTok{evaluate}\NormalTok{(y\_test, y\_pred\_logit\_all\_data, }\AttributeTok{border =}\NormalTok{ i)}
\NormalTok{  pre }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(pre, }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ res[}\DecValTok{2}\NormalTok{, }\StringTok{"prec"}\NormalTok{])}
\NormalTok{  tpr }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(tpr, res[}\DecValTok{2}\NormalTok{, }\StringTok{"sens"}\NormalTok{])}
\NormalTok{\}}

\NormalTok{TPR }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\FloatTok{0.1}\NormalTok{)}
\NormalTok{PREC }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{,}\FloatTok{0.1}\NormalTok{)}

\FunctionTok{plot}\NormalTok{(PREC, TPR, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{, }\AttributeTok{col=}\StringTok{"green"}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(pre, tpr, }\AttributeTok{type =} \StringTok{"s"}\NormalTok{, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{_main_files/figure-latex/unnamed-chunk-6-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(logit\_all\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = misstate ~ ., family = "binomial", data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.5406  -0.9641   0.1000   0.9745   5.5802  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -23.75983    0.50318 -47.219  < 2e-16 ***
## act          11.12570    1.41279   7.875 3.41e-15 ***
## ap            1.98894    0.49944   3.982 6.82e-05 ***
## at           10.80001    0.82025  13.167  < 2e-16 ***
## ceq          -5.65817    0.16604 -34.078  < 2e-16 ***
## che          -3.62274    0.73212  -4.948 7.49e-07 ***
## cogs         29.99856    1.55593  19.280  < 2e-16 ***
## csho          6.41974    0.27687  23.187  < 2e-16 ***
## dlc         -12.09188    0.76046 -15.901  < 2e-16 ***
## dltis        -2.55895    0.39298  -6.512 7.43e-11 ***
## dltt         17.01835    0.72114  23.599  < 2e-16 ***
## dp           -6.59251    0.65110 -10.125  < 2e-16 ***
## ib           -0.24714    0.13140  -1.881  0.06000 .  
## invt         -3.59020    0.53125  -6.758 1.40e-11 ***
## ivao          1.37159    0.43429   3.158  0.00159 ** 
## ivst         -2.32630    0.48566  -4.790 1.67e-06 ***
## lct          18.48910    1.38337  13.365  < 2e-16 ***
## lt          -35.46112    1.37644 -25.763  < 2e-16 ***
## ni            1.39985    0.12153  11.518  < 2e-16 ***
## ppegt        -6.34845    0.65147  -9.745  < 2e-16 ***
## pstk         -7.67877    0.35223 -21.800  < 2e-16 ***
## re            4.27251    0.05994  71.281  < 2e-16 ***
## rect          2.51693    0.80673   3.120  0.00181 ** 
## sale        -35.36934    1.76975 -19.986  < 2e-16 ***
## sstk          1.57673    0.23121   6.819 9.15e-12 ***
## txp          -3.70640    0.36741 -10.088  < 2e-16 ***
## txt          -0.55736    0.06326  -8.811  < 2e-16 ***
## xint          6.73976    0.38732  17.401  < 2e-16 ***
## prcc_f       12.19458    0.26323  46.327  < 2e-16 ***
## dch_wc       -1.97133    0.12683 -15.543  < 2e-16 ***
## ch_rsst       3.04583    0.15251  19.971  < 2e-16 ***
## dch_rec       1.61872    0.06807  23.779  < 2e-16 ***
## dch_inv       1.30387    0.06491  20.089  < 2e-16 ***
## soft_assets   2.03327    0.02586  78.635  < 2e-16 ***
## dpi          -0.49160    0.06295  -7.809 5.76e-15 ***
## ch_cs         0.98542    0.09663  10.198  < 2e-16 ***
## ch_cm        -0.57042    0.10192  -5.597 2.18e-08 ***
## ch_roa       -2.12709    0.12845 -16.559  < 2e-16 ***
## ch_fcf        1.91612    0.19656   9.748  < 2e-16 ***
## reoa         22.60563    0.59876  37.754  < 2e-16 ***
## EBIT         -3.30312    0.20996 -15.732  < 2e-16 ***
## issue         0.66310    0.02341  28.324  < 2e-16 ***
## bm           -0.39105    0.08582  -4.557 5.19e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 225660  on 162783  degrees of freedom
## Residual deviance: 184472  on 162741  degrees of freedom
## AIC: 184558
## 
## Number of Fisher Scoring iterations: 7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# 2. MODELL MIT ROH{-}DATEN TRANIEREN}
\NormalTok{splitted\_data }\OtherTok{\textless{}{-}} \FunctionTok{train\_test\_split\_smote}\NormalTok{(}\AttributeTok{data =}\NormalTok{ raw\_data, }
                                    \AttributeTok{y\_col =} \StringTok{\textquotesingle{}misstate\textquotesingle{}}\NormalTok{, }\AttributeTok{frac =} \FloatTok{0.7}\NormalTok{)}
\NormalTok{X\_train }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{X\_train}
\NormalTok{X\_test }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{X\_test}
\NormalTok{y\_train }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{y\_train}
\NormalTok{y\_test }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{y\_test}

\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ X\_train}
\NormalTok{train}\SpecialCharTok{$}\NormalTok{misstate }\OtherTok{\textless{}{-}}\NormalTok{ y\_train}

\NormalTok{logit\_raw\_data }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(misstate }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train, }
                      \AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\NormalTok{y\_pred\_logit\_raw\_data }\OtherTok{\textless{}{-}} \FunctionTok{predict.glm}\NormalTok{(logit\_raw\_data, X\_test, }
                                     \AttributeTok{type =} \StringTok{"response"}\NormalTok{ )}

\FunctionTok{evaluate}\NormalTok{(y\_test, y\_pred\_logit\_raw\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       0     1 total_acc   prec   sens     F1    F2    F.5
## 0 24616 10076       NaN    NaN    NaN    NaN   NaN    NaN
## 1   115   137    0.7084 0.0134 0.5437 0.0262 0.061 0.0166
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(logit\_raw\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = misstate ~ ., family = "binomial", data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.7581  -1.0411   0.0837   1.1359   3.3212  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  -1.04527    0.01501 -69.648  < 2e-16 ***
## act           8.96766    1.37982   6.499 8.08e-11 ***
## ap            5.63238    0.50424  11.170  < 2e-16 ***
## at           20.18742    0.94107  21.451  < 2e-16 ***
## ceq          -4.94838    0.15049 -32.882  < 2e-16 ***
## che          -7.32059    0.73408  -9.972  < 2e-16 ***
## cogs         25.58273    1.52700  16.754  < 2e-16 ***
## csho          5.23000    0.26738  19.560  < 2e-16 ***
## dlc          -9.52638    0.74396 -12.805  < 2e-16 ***
## dltis        -2.06434    0.36976  -5.583 2.37e-08 ***
## dltt         16.47397    0.72063  22.861  < 2e-16 ***
## dp           -8.53646    0.65377 -13.057  < 2e-16 ***
## ib           -0.05232    0.11352  -0.461 0.644863    
## invt         -1.16582    0.51706  -2.255 0.024151 *  
## ivao          0.96618    0.44619   2.165 0.030358 *  
## ivst         -1.72751    0.52092  -3.316 0.000912 ***
## lct          14.80146    1.36620  10.834  < 2e-16 ***
## lt          -38.23713    1.37870 -27.734  < 2e-16 ***
## ni            1.27856    0.10622  12.037  < 2e-16 ***
## ppegt       -13.45517    0.71929 -18.706  < 2e-16 ***
## pstk         -9.80106    0.36253 -27.035  < 2e-16 ***
## re            3.64993    0.05327  68.517  < 2e-16 ***
## rect          5.92899    0.79359   7.471 7.95e-14 ***
## sale        -31.50988    1.74106 -18.098  < 2e-16 ***
## sstk          1.79127    0.23203   7.720 1.16e-14 ***
## txp          -4.02413    0.36272 -11.094  < 2e-16 ***
## txt          -1.08172    0.05855 -18.475  < 2e-16 ***
## xint          5.42623    0.37607  14.429  < 2e-16 ***
## prcc_f       11.17267    0.23612  47.317  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 225660  on 162783  degrees of freedom
## Residual deviance: 205190  on 162755  degrees of freedom
## AIC: 205248
## 
## Number of Fisher Scoring iterations: 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# 3. MODELL MIT RATIO{-}DATEN TRANIEREN}
\NormalTok{splitted\_data }\OtherTok{\textless{}{-}} \FunctionTok{train\_test\_split\_smote}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ratio\_data, }
                                    \AttributeTok{y\_col =} \StringTok{\textquotesingle{}misstate\textquotesingle{}}\NormalTok{, }\AttributeTok{frac =} \FloatTok{0.7}\NormalTok{)}

\NormalTok{X\_train }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{X\_train}
\NormalTok{X\_test }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{X\_test}
\NormalTok{y\_train }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{y\_train}
\NormalTok{y\_test }\OtherTok{\textless{}{-}}\NormalTok{ splitted\_data}\SpecialCharTok{$}\NormalTok{y\_test}

\NormalTok{train }\OtherTok{\textless{}{-}}\NormalTok{ X\_train}
\NormalTok{train}\SpecialCharTok{$}\NormalTok{misstate }\OtherTok{\textless{}{-}}\NormalTok{ y\_train}

\NormalTok{logit\_ratio\_data }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(misstate }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train,}
                        \AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_pred\_logit\_ratio\_data }\OtherTok{\textless{}{-}} \FunctionTok{predict.glm}\NormalTok{(logit\_ratio\_data, X\_test,}
                                       \AttributeTok{type =} \StringTok{"response"}\NormalTok{ )}

\FunctionTok{evaluate}\NormalTok{(y\_test, y\_pred\_logit\_ratio\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       0     1 total_acc   prec   sens    F1     F2    F.5
## 0 20526 14166       NaN    NaN    NaN   NaN    NaN    NaN
## 1    91   161     0.592 0.0112 0.6389 0.022 0.0523 0.0139
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(logit\_ratio\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = misstate ~ ., family = "binomial", data = train)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -3.156  -1.083   0.527   1.013   6.492  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -35.74613    0.56539 -63.224  < 2e-16 ***
## dch_wc       -2.84428    0.12496 -22.762  < 2e-16 ***
## ch_rsst       7.46171    0.14249  52.366  < 2e-16 ***
## dch_rec       1.77243    0.06538  27.110  < 2e-16 ***
## dch_inv       1.16130    0.06228  18.645  < 2e-16 ***
## soft_assets   1.99318    0.02350  84.816  < 2e-16 ***
## dpi          -0.25691    0.05951  -4.317 1.58e-05 ***
## ch_cs         1.00253    0.09622  10.419  < 2e-16 ***
## ch_cm        -1.03301    0.09577 -10.787  < 2e-16 ***
## ch_roa       -4.42682    0.13154 -33.653  < 2e-16 ***
## ch_fcf        9.45417    0.18045  52.392  < 2e-16 ***
## reoa         33.46111    0.67111  49.860  < 2e-16 ***
## EBIT         -4.38108    0.22272 -19.670  < 2e-16 ***
## issue         0.93498    0.02250  41.562  < 2e-16 ***
## bm           -1.20690    0.07980 -15.124  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 225660  on 162783  degrees of freedom
## Residual deviance: 200018  on 162769  degrees of freedom
## AIC: 200048
## 
## Number of Fisher Scoring iterations: 7
\end{verbatim}

\startappendices

\hypertarget{the-first-appendix}{%
\chapter{The First Appendix}\label{the-first-appendix}}

This first appendix includes an R chunk that was hidden in the document (using \texttt{echo\ =\ FALSE}) to help with readibility:

\textbf{In 02-rmd-basics-code.Rmd}

\textbf{And here's another one from the same chapter, i.e.~Chapter \ref{code}:}

\hypertarget{the-second-appendix-for-fun}{%
\chapter{The Second Appendix, for Fun}\label{the-second-appendix-for-fun}}


%%%%% REFERENCES
\setlength{\baselineskip}{0pt} % JEM: Single-space References

{\renewcommand*\MakeUppercase[1]{#1}%
\printbibliography[heading=bibintoc,title={\bibtitle}]}


\end{document}
