---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
bib-humanities: true
documentclass: book
bibliography: references.bib
---

# Datensatz {#dataset} 
Ein besonderer Fokus liegt aus diesem Grund auf der Minimierung des Typ II Fehlers respektive der Maximierung der Sensitivität des Modells. Eine höchstmögliche Genauigkeit ist gut, aufgrund der sehr geringen Anzahl an bilanziellen Verfehlungen verglichen zu der Stichprobengröße sollte hier allerdings kein Schwerpunkt liegen.
Zum Training und Test des Machine Learning Modells, wird ein Datensatz aus der Veröffentlichung von Bao et al. (2020) verwendet. Die Autoren haben diesen im Internet auf der Seite „GitHub“ zur Verfügung gestellt (vgl. Bao et al. 2020, GitHub Repository). Dieser besteht aus allen öffentlich gelisteten US-amerikanischen Firmen im Zeitraum von 1991 bis 2008. Die Accounting-Betrugsfälle aus den „Accounting and Auditing Enforcement Releases“ (AAER), die von der United States Securities and Exchange Commission (SEC) im gleichen Zeitraum veröffentlicht worden sind (vgl. Bao et al. 2020, S. 207). Der Datensatz listet für jeden Eintrag 28 verschiedene finanzielle Items auf. Diese setzen sich aus den Veröffentlichungen von Cecchini et al. (2010) und Dechow et al. (2011) zusammen. Die finanziellen Items stammen aus vier verschiedenen Bereichen, der Bilanz (z.B. gesamte Forderungen), der Gewinn- und Verlustrechnung (z.B. Nettoumsatz), der Kapitalflussrechnung (z.B. Langzeitemission von Schuldtiteln) und dem Marktwert (z.B. Common Shares Outstanding).
Da Accounting-Betrugsfälle eher seltener vorkommen (vgl. Dutta et al. 2011, S. 381), weist der Datensatz eine große Verteilungsungleichheit zwischen den Betrugs- und Nicht-betrugsfällen auf. Weniger als ein Prozent aller Einträge im Datensatz sind hierbei Betrugsfälle. Weitere Betrugsfälle könnten über andere Datenbanken gesucht werden, hierbei gibt es allerdings das Problem, dass die Betrugsfälle identifiziert und einzeln herausgesucht werden müssen. Daher besteht hier ein „class imbalance“ Problem, das mithilfe des Machine Learning Algorithmus gelöst werden muss.
Zudem sind nur Betrugsfälle bis zum Jahr 2008 in diesem Datensatz erhalten, da die SEC nach der Finanzkrise ihre Prioritäten geändert hat (vgl. Bao et al. 2020, S. 208). Sollte sich die Art und Weise mit der Accounting-Betrug durchgeführt wird in den Jahren danach geändert haben, so können diese Fälle unter Umständen nicht vom Algorithmus erkannt werden.


 AB HIER HANDELT SICH DER AUFSCHRIEB MEHR UM NOTIZEN ALS UM EINE ABGABEFÄHIGE VERSION.
 
 
 
```{r, include=FALSE}
# install.packages("stringi", type="binary")
# install.packages('caret')
# install.packages('neuralnet')
# install.packages('dplyr')
# install.packages('Hmisc')
# install.packages('smotefamily')
# install.packages('readr')
# install.packages('rio', type='binary')
# install.packages('bookdown', type='binary')
# install.packages('kdensity')
# install.packages('highcharter')

library(highcharter)
library(neuralnet)
library(caret)
library(dplyr)
library(Hmisc)
library(smotefamily)
library(readr)
library(rio)
library(kdensity)
```
Cleaning-Process-Reihenfolge:

  1. Daten als "data" Laden
  2. Spalten "p_aaer" und "new_p_aaer" löschen
  3. Alle Zeilen mit NaN-Werten löschen
  4. all_data bilden: Besteht nur aus 14 + 28 + 2 Vars (ratios, raw, misstate, fyear)
  5. all_data via Jahreszahl normalisieren. fyear droppen. 
  6. raw_data (28 + 1 Vars) und  ratio_data (14 + 1 Vars) aus all_data bilden
  7. deskriptive Statistiken können mit den Datensätzen: min, Max, Mean, Median 0.25 und 0.75 Quantile UND normalisierte Boxplots und Histogramme. Ggf noch Ausreißer raus.

  
Dann ist die Datenvorbereitung fertig und man kann Modelle damit rechnen. Bis zu dem Punkt geht die nächste Zelle:

```{r}
## 1. Daten als "data" Laden
data <- import("data/uscecchini28.csv")

## 2. Spalten "p_aaer" und "new_p_aaer" löschen
data <- data[,-match(c("p_aaer", "new_p_aaer"), names(data))]

## 3. Alle Zeilen mit NaN-Werten löschen
data <- data[complete.cases(data),]

## 4. all_data bilden: Besteht nur aus 14 + 28 + 2 Vars
all_names <- c("fyear", "misstate", "act", "ap", "at", "ceq", "che", 
               "cogs", "csho", "dlc", "dltis", "dltt", "dp", "ib", 
               "invt", "ivao", "ivst", "lct", "lt", "ni", "ppegt", 
               "pstk", "re", "rect", "sale", "sstk", "txp", "txt", 
               "xint", "prcc_f", "dch_wc", "ch_rsst", "dch_rec", 
               "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", 
               "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")
all_data <- data[, match(all_names, names(data))]

## 5. all_data via Jahreszahl normalisieren. fyear danach droppen
# Funktion schreiben
normalize <- function(x){
  return((x - min(x)) / (max(x) - min(x)))
}
# Funktion anwenden
for (year in unique(all_data$fyear)){
  for (col in names(all_data)){
    all_data[data$fyear == year, col] <- normalize(
      all_data[data$fyear == year, col]
      ) 
  }
}
all_data <- all_data[, -1] 

## 6. raw_data (28 + 1 Vars) und  ratio_data (14 + 1 Vars) aus all_data bilden
raw_names <- c("misstate", "act", "ap", "at", "ceq", "che", "cogs", 
               "csho", "dlc", "dltis", "dltt", "dp", "ib", "invt", 
               "ivao", "ivst", "lct", "lt", "ni", "ppegt", "pstk", 
               "re", "rect", "sale", "sstk", "txp", "txt", "xint", 
               "prcc_f")

ratio_names <- c("misstate", "dch_wc", "ch_rsst", "dch_rec", 
                 "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", 
                 "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")

raw_data <- all_data[, match(raw_names, names(all_data))]
ratio_data <- all_data[, match(ratio_names, names(all_data))]

# ==================================================================| Maximale Breite bis zum Strich

## 7. Statistiken erstellen: min, Max, Mean, Median 0.25- und 0.75- 
## Quantile, normalisierte Boxplots und Histogramme mir KDE. 

# Ggf noch Ausreißer raus.
# SKIP. 


```

Zunächst schauen wir, wie viele Betrugsfälle wir anteilig haben. Das sind super wenige, was man in dem Pie Chart sieht. Es reicht allerdings vollkommen aus das zu sagen, statt einen Chart rein zu hauen, der kaum Infos enthält. Daher "include = False"
```{r, include = False}
# Pie chart missstate 

labels <- c("fraud", "no fraud")
fraud <- sum(all_data$misstate)
no_fraud <- nrow(all_data) - sum(all_data$misstate)
fraud_smote <- no_fraud
pie(c(fraud, no_fraud), labels = labels, main="Anteil Betrugsfälle vor SMOTE", init.angle = 90, clockwise = T)

```
ERSTE GRAFIK: 

  Enthält 3 infos:
    -> Anteil Betrugsfälle ist sehr gering. Wir brauchen daher SMOTE, was in Kap 4 erklärt wird.
    -> Anzahl untersuchter Berichte schwankt nur schwach unter den Jahren
    -> Anteil identifizierter Betrugsfälle schwankt stark unter den Jahren. Wir treffen allerdings die Annahme, dass Betrug inherent gleich aufgebaut ist und dass das daher vernachlässigbar ist. Was man hier sieht ist vielmehr ein Trend. Nimmt in den letzten Betrachtungsjahren massiv ab, was wahrscheinlich daher kommt, dass zum Zeitpunkt der Datenerhebung der Betrug innerhalb dieser Samples noch nicht vollständig aufgedeckt war. Soll heißen: Je älter ein Betrachtungszeitraum ist, desto wahrscheinlicher ist es wohl, dass der darin enthaltene Betrug aufgedeckt wurde. 

Wir schauen uns Berichte pro Jahr an, um ein Gefühl dafür zu bekommen, ob einzele Jahre stark über- oder unterrepräsentiert sind. In der gleichen Grafik schauen wir nach, wie viele der Untersuchten Berichte sich als Fraud herausgestellt haben. Wir wollen wissen, ob wir einzelne Jahre grundlegend anders behandeln müssen. 

Cunclusion: Ne, passt eigenlich. 2012 gabs nur 3706 Berichte, die es in unseren Datensatz geschafft haben und aus 1997 halt 6110. Gründe könnten in der Art liegen, wie unsere Daten erhoben wurden. Alex, wenn dir da was kleveres einfällt: Hau raus. Muss aber nicht zwingend sein, weil kein Jahr massiv unter- oder überrepräsentiert wird. Die Rate der Gefunden Berichte Vervierfacht sich zwischen 1995 und 2000. Warum, weiß ich nicht. 2009 ist dann nochmal ein kleiner Peak nach oben, wahrscheinlich, weil nach der Finanzkrise genauer hingeschaut wurde. 

```{r}
counts <- table(data$fyear)
counts_misstate <- table(data$misstate, data$fyear)
counts_misstate_rel <- round(counts_misstate[2,] / (counts_misstate[1,] +counts_misstate[2,]), 4)

highchart() %>% 
  hc_yAxis_multiples(
    list(lineWidth = 3, lineColor='grey', title=list(text="Anzahl Berichte")),
    list(lineWidth = 3, lineColor="red", title=list(text="Anteil Betrugsfälle"))
  ) %>% 
  hc_add_series(data = counts, color='grey', type = "column") %>% 
  hc_add_series(data = as.table(counts_misstate_rel), color='red', type = "column", yAxis = 1) %>%
  hc_xAxis(categories = unique(data$fyear), title = list(text = "Jahre"))
```

ZWEITE GRAFIK:
  -> Sagt aus, dass wir nach Jahr normalisieren müssen, weil Jahre sonst nicht vergliechbar. 

Nachfolgend noch ein graph, mit dem wir belegen, dass eine Normalisierung nach fyear nötig war. Durchschnitt von sale pro Jahr. 

Der Graph zeigt, dass die durchschnittlichen Sales, cogs und total assets sich in den Jahren '90 bis 2012 verfünffacht haben. Der Unterschied ist waaaay too big, als dass ein ML-Modell damit klarkommen würde. Generelles Marktwachstum und Inflation haben die Zahlen von jüngeren Jahren aufgebläht. Daher die 0-1-Normalization, um die unterschiedlichen Jahre vergleichbar zu machen. 


```{r}
#sales
placeholder_mean_sales <- numeric(length(unique(data$fyear)))
counter = 0
for (i in unique(data$fyear)){
  counter = counter + 1
  placeholder_mean_sales[counter] <- mean(data[data$fyear == i,]$sale)
}
names(placeholder_mean_sales) <- unique(data$fyear)

# cogs
placeholder_mean_cogs <- numeric(length(unique(data$fyear)))
counter = 0
for (i in unique(data$fyear)){
  counter = counter + 1
  placeholder_mean_cogs[counter] <- mean(data[data$fyear == i,]$cogs)
}
names(placeholder_mean_cogs) <- unique(data$fyear)

# at (assets total)
placeholder_mean_at <- numeric(length(unique(data$fyear)))
counter = 0
for (i in unique(data$fyear)){
  counter = counter + 1
  placeholder_mean_at[counter] <- mean(data[data$fyear == i,]$at)
}
names(placeholder_mean_at) <- unique(data$fyear)

highchart() %>% 
  hc_yAxis_multiples(
    list(lineWidth = 3, lineColor='black', title=list(text="Durchschnitt sales")),
    list(lineWidth = 3, lineColor="grey", title=list(text="Durchschnitt cogs")),
    list(lineWidth = 3, lineColor="silver", title=list(text="Durchschnitt total Assets"))
  ) %>% 
  hc_add_series(data = as.table(placeholder_mean_sales), color='black', type = "column") %>% 
  hc_add_series(data = as.table(placeholder_mean_cogs), color='grey', type = "column", yAxis = 1) %>%
  hc_add_series(data = as.table(placeholder_mean_at), color='silver', type = "column", yAxis = 2) %>%
  hc_xAxis(categories = unique(data$fyear), title = list(text = "Jahre"))
```


Hier könnte man jetzt für jede Var einen Boxplot machen um ihre Verteilung zu zeigen. 
Allerdings sieht man da nicht viel, weil die Quantile ja auch normalisiert sind. 
Man kann nur eine Grobe Verteilungsform aus den Graphen ableiten. 
Die Durchschnitte nach Jahr in der Grafik oben stehen exemplarisch für alle 28 Rohdaten.
Die 14 ratios könnte man eventuell noch am ehsten in Boxplots darstellen. 


```{r}
summary(data[, match(ratio_names, names(data))])
```

```{r}
summary(ratio_data)
```















