---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
bib-humanities: true
documentclass: book
bibliography: references.bib
---

# Datensatz {#dataset} 
Ein besonderer Fokus liegt aus diesem Grund auf der Minimierung des Typ II Fehlers respektive der Maximierung der Sensitivität des Modells. Eine höchstmögliche Genauigkeit ist gut, aufgrund der sehr geringen Anzahl an bilanziellen Verfehlungen verglichen zu der Stichprobengröße sollte hier allerdings kein Schwerpunkt liegen.
Zum Training und Test des Machine Learning Modells, wird ein Datensatz aus der Veröffentlichung von Bao et al. (2020) verwendet. Die Autoren haben diesen im Internet auf der Seite „GitHub“ zur Verfügung gestellt (vgl. Bao et al. 2020, GitHub Repository). Dieser besteht aus allen öffentlich gelisteten US-amerikanischen Firmen im Zeitraum von 1991 bis 2008. Die Accounting-Betrugsfälle aus den „Accounting and Auditing Enforcement Releases“ (AAER), die von der United States Securities and Exchange Commission (SEC) im gleichen Zeitraum veröffentlicht worden sind (vgl. Bao et al. 2020, S. 207). Der Datensatz listet für jeden Eintrag 28 verschiedene finanzielle Items auf. Diese setzen sich aus den Veröffentlichungen von Cecchini et al. (2010) und Dechow et al. (2011) zusammen. Die finanziellen Items stammen aus vier verschiedenen Bereichen, der Bilanz (z.B. gesamte Forderungen), der Gewinn- und Verlustrechnung (z.B. Nettoumsatz), der Kapitalflussrechnung (z.B. Langzeitemission von Schuldtiteln) und dem Marktwert (z.B. Common Shares Outstanding).
Da Accounting-Betrugsfälle eher seltener vorkommen (vgl. Dutta et al. 2011, S. 381), weist der Datensatz eine große Verteilungsungleichheit zwischen den Betrugs- und Nicht-betrugsfällen auf. Weniger als ein Prozent aller Einträge im Datensatz sind hierbei Betrugsfälle. Weitere Betrugsfälle könnten über andere Datenbanken gesucht werden, hierbei gibt es allerdings das Problem, dass die Betrugsfälle identifiziert und einzeln herausgesucht werden müssen. Daher besteht hier ein „class imbalance“ Problem, das mithilfe des Machine Learning Algorithmus gelöst werden muss.
Zudem sind nur Betrugsfälle bis zum Jahr 2008 in diesem Datensatz erhalten, da die SEC nach der Finanzkrise ihre Prioritäten geändert hat (vgl. Bao et al. 2020, S. 208). Sollte sich die Art und Weise mit der Accounting-Betrug durchgeführt wird in den Jahren danach geändert haben, so können diese Fälle unter Umständen nicht vom Algorithmus erkannt werden.


 AB HIER HANDELT SICH DER AUFSCHRIEB MEHR UM NOTIZEN ALS UM EINE ABGABEFÄHIGE VERSION.
 
 
 
```{r, include=FALSE}
# install.packages("stringi", type="binary")
# install.packages('caret')
# install.packages('neuralnet')
# install.packages('dplyr')
# install.packages('Hmisc')
# install.packages('smotefamily')
# install.packages('readr')
# install.packages('rio', type='binary')
# install.packages('bookdown', type='binary')
# install.packages('kdensity')

library(neuralnet)
library(caret)
library(dplyr)
library(Hmisc)
library(smotefamily)
library(readr)
library(rio)
library(kdensity)
```
Cleaning-Prozess-Reichenfolge:

  1. Daten als "data" Laden
  2. Spalten "p_aaer" und "new_p_aaer" löschen
  3. Alle Zeilen mit NaN-Werten löschen
  4. all_data bilden: Besteht nur aus 14 + 28 + 2 Vars 
  5. all_data via Jahreszahl normalisieren. fyear droppen. 
  6. raw_data (28 + 1 Vars) und  ratio_data (14 + 1 Vars) aus all_data bilden
  7. deskriptive Statistiken können mit den Datensätzen: min, Max, Mean, Median 0.25 und 0.75 Quantile UND normalisierte Boxplots und Histogramme. Ggf noch Ausreißer raus.

  
Dann ist die Datenvorbereitung fertig und man kann Modelle damit rechnen. Bis zu dem Punkt geht die nächste Zelle:

```{r}
## 1. Daten als "data" Laden
data <- import("data/uscecchini28.csv")

## 2. Spalten "p_aaer" und "new_p_aaer" löschen
data <- data[,-match(c("p_aaer", "new_p_aaer"), names(data))]

## 3. Alle Zeilen mit NaN-Werten löschen
data <- data[complete.cases(data),]

## 4. all_data bilden: Besteht nur aus 14 + 28 + 2 Vars
all_names <- c("fyear", "misstate", "act", "ap", "at", "ceq", "che", 
               "cogs", "csho", "dlc", "dltis", "dltt", "dp", "ib", 
               "invt", "ivao", "ivst", "lct", "lt", "ni", "ppegt", 
               "pstk", "re", "rect", "sale", "sstk", "txp", "txt", 
               "xint", "prcc_f", "dch_wc", "ch_rsst", "dch_rec", 
               "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", 
               "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")
all_data <- data[, match(all_names, names(data))]

## 5. all_data via Jahreszahl normalisieren. fyear danach droppen
# Funktion schreiben
normalize <- function(x){
  return((x - min(x)) / (max(x) - min(x)))
}
# Funktion anwenden
for (year in unique(all_data$fyear)){
  for (col in names(all_data)){
    all_data[data$fyear == year, col] <- normalize(
      all_data[data$fyear == year, col]
      ) 
  }
}
all_data <- all_data[, -1] 

## 6. raw_data (28 + 1 Vars) und  ratio_data (14 + 1 Vars) aus all_data bilden
raw_names <- c("misstate", "act", "ap", "at", "ceq", "che", "cogs", 
               "csho", "dlc", "dltis", "dltt", "dp", "ib", "invt", 
               "ivao", "ivst", "lct", "lt", "ni", "ppegt", "pstk", 
               "re", "rect", "sale", "sstk", "txp", "txt", "xint", 
               "prcc_f")

ratio_names <- c("misstate", "dch_wc", "ch_rsst", "dch_rec", 
                 "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", 
                 "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")

raw_data <- all_data[, match(raw_names, names(all_data))]
ratio_data <- all_data[, match(ratio_names, names(all_data))]

# ==================================================================| Maximale Breite bis zum Strich

## 7. Statistiken erstellen: min, Max, Mean, Median 0.25- und 0.75- 
## Quantile, normalisierte Boxplots und Histogramme mir KDE. 

# Ggf noch Ausreißer raus.
# SKIP. 


```

Zunächst schauen wir, wie viele Betrugsfälle wir anteilig haben. Das sind super wenige, was man in dem Pie Chart sieht.
```{r}
# Pie chart missstate 

labels <- c("fraud", "no fraud")
fraud <- sum(all_data$misstate)
no_fraud <- nrow(all_data) - sum(all_data$misstate)
fraud_smote <- no_fraud
pie(c(fraud, no_fraud), labels = labels, main="Anteil Betrugsfälle vor SMOTE", init.angle = 90, clockwise = T)

```

Wir schauen uns Berichte pro Jahr an, um ein Gefühl dafür zu bekommen, ob ein Jahr über- oder unterrepräsentiert wird. 
Cunclusion: Ja, passt eigenlich.2012 gabs nur 3706 Berichte, die es in unseren Datensatz geschafft haben und aus 1997 halt 6110. Gründe könnten in der Art liegen, wie unsere Daten erhoben wurden. Alex, wenn dir da was kleveres einfällt: Hau raus. Muss aber nicht zwingend sein, weil kein Jahr massiv unter- oder überrepräsentiert wird. Interessanter wird der nächste Punkt...
```{r}
# Simple Bar Plot
counts <- table(data$fyear)
barplot(counts, main="Berichte pro Jahr",
   xlab="")
```
Nachfolgenden sehen wir die erkannten Betrugsfälle für jedes Jahr. Das ist schon eine etwas andere Hausnummer. In der Grafik darunter wird das genauer analysiert. 

```{r}
counts_misstate <- table(data$misstate, data$fyear)
barplot(counts_misstate[2,], main="Betrugsfälle pro Jahr",
   xlab="")
```
Zwischen 99 und 2004 wurden beinahe 3x so viele Betrugsfälle identifiziert wie in den anderen Perioden. Man könnte vermuten, dass davor nicht anständig hingeschaut wurde und danach (z.B. wegen der dot-com-krise) Kontrollmaßnahmen verstärkt wurden. Ist aber reine Spekualtion und Alex oder MArc können da sicher mehr dazu sagen. 
```{r}
counts_misstate_rel <- counts_misstate[2,] / (counts_misstate[1,] +counts_misstate[2,])
barplot(counts_misstate_rel, main="Anteil Betrugsfälle pro Jahr",
   xlab="")
```
Nachfolgend noch ein graph, mit dem wir belegen, dass eine Normalisierung nach fyear nötig war. Durchschnitt von sale pro Jahr. 

```{r}

placeholder <- numeric(length(unique(data$fyear)))
counter = 0
for (i in unique(data$fyear)){
  counter = counter + 1
  placeholder[counter] <- mean(data[data$fyear == i,]$sale)
}
names(placeholder) <- unique(data$fyear)

barplot(placeholder, main="Mean sale pro Jahr",
   xlab="")
```
Der Graph zeigt, dass die durchschnittlichen Sales sich in den Jahren '99 bis 2012 vervierfacht haben. Der Unterschied ist waaaay too big, als dass ein ML-;Modell damit klarkommen würde. Dahe rdie 0-1-Normalization, da wir sonst anhand der Jahreszahlen unterschiedliche Ergebnisse bekommen würden 
























