---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
bib-humanities: true
documentclass: book
bibliography: references.bib
---

# Datensatz {#dataset} 




 AB HIER HANDELT SICH DER AUFSCHRIEB MEHR UM NOTIZEN ALS UM EINE ABGABEFÄHIGE VERSION.
 
 
 
```{r, include=FALSE}
# install.packages("stringi", type="binary")
# install.packages('caret')
# install.packages('neuralnet')
# install.packages('dplyr')
# install.packages('Hmisc')
# install.packages('smotefamily')
# install.packages('readr')
# install.packages('rio', type='binary')
# install.packages('bookdown', type='binary')
# install.packages('kdensity')
# install.packages('highcharter')

library(highcharter)
library(neuralnet)
library(caret)
library(dplyr)
library(Hmisc)
library(smotefamily)
library(readr)
library(rio)
library(kdensity)
```
Cleaning-Process-Reihenfolge:

  1. Daten als "data" Laden. 146.000 Beobachtungen
  2. Spalten "p_aaer" und "new_p_aaer" löschen, weil sie fast nur aus NaN bestehen. Restliche Vars haben max. 10% NaN-Werte.
  3. Alle Zeilen mit NaN-Werten löschen 116.478 Beobachtungen bleiben übrig. 
  4. all_data bilden: Besteht nur aus 14 + 28 + 2 Vars (ratios, raw, misstate & fyear)
  5. all_data via Jahreszahl normalisieren. fyear danach löschen 
  6. raw_data (28 + 1 Vars) und  ratio_data (14 + 1 Vars) aus all_data bilden
  7. Explorative Datenanalyse 

  
Dann ist die Datenvorbereitung fertig und man kann Modelle damit rechnen. Bis zu dem Punkt geht die nächste Zelle:

```{r}
## 1. Daten als "data" Laden
data <- import("data/uscecchini28.csv")

## 2. Spalten "p_aaer" und "new_p_aaer" löschen
data <- data[,-match(c("p_aaer", "new_p_aaer"), names(data))]

## 3. Alle Zeilen mit NaN-Werten löschen
data <- data[complete.cases(data),]

## 4. all_data bilden: Besteht nur aus 14 + 28 + 2 Vars
all_names <- c("fyear", "misstate", "act", "ap", "at", "ceq", "che", 
               "cogs", "csho", "dlc", "dltis", "dltt", "dp", "ib", 
               "invt", "ivao", "ivst", "lct", "lt", "ni", "ppegt", 
               "pstk", "re", "rect", "sale", "sstk", "txp", "txt", 
               "xint", "prcc_f", "dch_wc", "ch_rsst", "dch_rec", 
               "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", 
               "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")
all_data <- data[, match(all_names, names(data))]

## 5. all_data via Jahreszahl normalisieren. fyear danach droppen
# Funktion schreiben
normalize <- function(x){
  return((x - min(x)) / (max(x) - min(x)))
}
# Funktion anwenden
for (year in unique(all_data$fyear)){
  for (col in names(all_data)){
    all_data[data$fyear == year, col] <- normalize(
      all_data[data$fyear == year, col]
      ) 
  }
}
all_data <- all_data[, -1] 

## 6. raw_data (28 + 1 Vars) und  ratio_data (14 + 1 Vars) aus all_data bilden
raw_names <- c("misstate", "act", "ap", "at", "ceq", "che", "cogs", 
               "csho", "dlc", "dltis", "dltt", "dp", "ib", "invt", 
               "ivao", "ivst", "lct", "lt", "ni", "ppegt", "pstk", 
               "re", "rect", "sale", "sstk", "txp", "txt", "xint", 
               "prcc_f")

ratio_names <- c("misstate", "dch_wc", "ch_rsst", "dch_rec", 
                 "dch_inv", "soft_assets", "dpi", "ch_cs", "ch_cm", 
                 "ch_roa", "ch_fcf", "reoa", "EBIT", "issue", "bm")

raw_data <- all_data[, match(raw_names, names(all_data))]
ratio_data <- all_data[, match(ratio_names, names(all_data))]

# ==================================================================| Maximale Breite bis zum Strich

## 7. Statistiken erstellen


```

Zunächst schauen wir, wie viele Betrugsfälle wir anteilig haben. Das sind super wenige, was man in dem Pie Chart sieht. Es reicht allerdings vollkommen aus das zu sagen, statt einen Chart rein zu hauen, der kaum Infos enthält. Daher "include = False"
```{r, include = False}
# Pie chart missstate 

labels <- c("fraud", "no fraud")
fraud <- sum(all_data$misstate)
no_fraud <- nrow(all_data) - sum(all_data$misstate)
fraud_smote <- no_fraud
pie(c(fraud, no_fraud), labels = labels, main="Anteil Betrugsfälle vor SMOTE", init.angle = 90, clockwise = T)

```
ERSTE GRAFIK: 


Wir schauen uns Berichte pro Jahr an, um ein Gefühl dafür zu bekommen, ob einzele Jahre stark über- oder unterrepräsentiert sind. In der gleichen Grafik schauen wir nach, wie viele der Untersuchten Berichte sich als Fraud herausgestellt haben. Wir wollen wissen, ob wir einzelne Jahre grundlegend anders behandeln müssen. 

    -> Anteil Betrugsfälle ist sehr gering. Wir brauchen daher SMOTE, was in Kap 4 erklärt wird.
    -> Anzahl untersuchter Berichte schwankt nur schwach unter den Jahren
    -> Anteil identifizierter Betrugsfälle schwankt stark unter den Jahren. Wir treffen allerdings die Annahme, dass Betrug inherent gleich aufgebaut ist und dass das daher vernachlässigbar ist. Was man hier sieht ist vielmehr ein Trend. Nimmt in den letzten Betrachtungsjahren massiv ab, was wahrscheinlich daher kommt, dass zum Zeitpunkt der Datenerhebung der Betrug innerhalb dieser Samples noch nicht vollständig aufgedeckt war. Soll heißen: Je älter ein Betrachtungszeitraum ist, desto wahrscheinlicher ist es wohl, dass der darin enthaltene Betrug aufgedeckt wurde.

Cunclusion: Ne, passt eigenlich. 2012 gabs nur 3706 Berichte, die es in unseren Datensatz geschafft haben und aus 1997 halt 6110. Gründe könnten in der Art liegen, wie unsere Daten erhoben wurden. Alex, wenn dir da was kleveres einfällt: Hau raus. Muss aber nicht zwingend sein, weil kein Jahr massiv unter- oder überrepräsentiert wird. Die Rate der Gefunden Berichte Vervierfacht sich zwischen 1995 und 2000. Warum, weiß ich nicht. 2009 ist dann nochmal ein kleiner Peak nach oben, wahrscheinlich, weil nach der Finanzkrise genauer hingeschaut wurde. 

```{r}
counts <- table(data$fyear)
counts_misstate <- table(data$misstate, data$fyear)
counts_misstate_rel <- round(counts_misstate[2,] / (counts_misstate[1,] +counts_misstate[2,]), 4)

highchart() %>% 
  hc_yAxis_multiples(
    list(lineWidth = 3, lineColor='grey', title=list(text="Anzahl Berichte")),
    list(lineWidth = 3, lineColor="black", title=list(text="Anteil Betrugsfälle"))
  ) %>% 
  hc_add_series(data = counts, color='grey', type = "column") %>% 
  hc_add_series(data = as.table(counts_misstate_rel), color='black', type = "column", yAxis = 1) %>%
  hc_xAxis(categories = unique(data$fyear), title = list(text = "Jahre"))
```

ZWEITE GRAFIK:
  -> Sagt aus, dass wir nach Jahr normalisieren müssen, weil Jahre sonst nicht vergliechbar. 

Nachfolgend noch ein graph, mit dem wir belegen, dass eine Normalisierung nach fyear nötig war. Durchschnitt von sale pro Jahr. 

Der Graph zeigt, dass die durchschnittlichen Sales, cogs und total assets sich in den Jahren '90 bis 2012 verfünffacht haben. Der Unterschied ist waaaay too big, als dass ein ML-Modell damit klarkommen würde. Generelles Marktwachstum und Inflation haben die Zahlen von jüngeren Jahren aufgebläht. Daher die 0-1-Normalization, um die unterschiedlichen Jahre vergleichbar zu machen. 


```{r}
#sales
placeholder_mean_sales <- numeric(length(unique(data$fyear)))
counter = 0
for (i in unique(data$fyear)){
  counter = counter + 1
  placeholder_mean_sales[counter] <- mean(data[data$fyear == i,]$sale)
}
names(placeholder_mean_sales) <- unique(data$fyear)

# cogs
placeholder_mean_cogs <- numeric(length(unique(data$fyear)))
counter = 0
for (i in unique(data$fyear)){
  counter = counter + 1
  placeholder_mean_cogs[counter] <- mean(data[data$fyear == i,]$cogs)
}
names(placeholder_mean_cogs) <- unique(data$fyear)

# at (assets total)
placeholder_mean_at <- numeric(length(unique(data$fyear)))
counter = 0
for (i in unique(data$fyear)){
  counter = counter + 1
  placeholder_mean_at[counter] <- mean(data[data$fyear == i,]$at)
}
names(placeholder_mean_at) <- unique(data$fyear)

highchart() %>% 
  hc_yAxis_multiples(
    list(lineWidth = 3, lineColor='black', title=list(text="Durchschnitt sales")),
    list(lineWidth = 3, lineColor="grey", title=list(text="Durchschnitt cogs")),
    list(lineWidth = 3, lineColor="silver", title=list(text="Durchschnitt total Assets"))
  ) %>% 
  hc_add_series(data = as.table(placeholder_mean_sales), color='black', type = "column") %>% 
  hc_add_series(data = as.table(placeholder_mean_cogs), color='grey', type = "column", yAxis = 1) %>%
  hc_add_series(data = as.table(placeholder_mean_at), color='silver', type = "column", yAxis = 2) %>%
  hc_xAxis(categories = unique(data$fyear), title = list(text = "Jahre"))
```


Hier könnte man jetzt für jede Var einen Boxplot machen um ihre Verteilung zu zeigen. 
Allerdings sieht man da nicht viel, weil die Quantile ja auch normalisiert sind. 
Man kann nur eine Grobe Verteilungsform aus den Graphen ableiten. 
Die Durchschnitte nach Jahr in der Grafik oben stehen exemplarisch für alle 28 Rohdaten.
Die 14 ratios könnte man eventuell noch am ehsten in Boxplots darstellen. 

```{r}
summary(all_data)
```

```{r}
summary(raw_data)
```

```{r}
summary(ratio_data)
```















